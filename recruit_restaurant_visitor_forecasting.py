# -*- coding: utf-8 -*-
"""Recruit_restaurant_visitor_forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YKBKiGP_90aT4ge7crfSL-yMkVTUPYWd

# Curriculum

1.  Setup Environment
2.  Load Dataset
3.  Understand the Data
4.  Data Prerocessing for EDA
5.  EDA
6.  Feature Engineering
7.  EDA part 2
8.  Statistical Significance Test
9.  Feature Encoding
10. Data Preprocessing for Model Building
11. Evaluation Metrics - Regression
12. Time Series Forecasting - ARIMA
13. Time Series Forecasting - Prophet
14. XGBoost Regression
15. XGBoost
16. CatBoost
17. LightGBM
18. Improve the Model - Hyperparameter Tuning
19. Ensemble (stacking)
20. Final RMSLE score and the recommendations

## **0. Setup Environment**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>Setup Environment</h2>
</div>

The goal of this section is to:
- Import all the packages
- Set the options for data visualizations
"""

# from google.colab import drive
# drive.mount('/content/drive', force_remount=True)

pip install --upgrade pip setuptools

pip install numpy==1.17.5

python -m venv myenv
myenv\Scripts\activate

python -m venv myenv
source myenv/bin/activate  # On Windows: myenv\Scripts\activate
pip install -r requirements.txt

pip install -r Requirements.txt

!pip install catboost
!pip install pmdarima
!pip install joypy
!pip install prophet

# ls drive/MyDrive/

cd /content/drive/MyDrive/09_Mega_Case_Study_Course/3_Restaurant\ Visitor\ Forecasting

# Commented out IPython magic to ensure Python compatibility.
# Data Manipulation
import numpy as np
import pandas as pd
# import pandas_profiling

# Data Visualization
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.lines as mlines

# Track time
import time
import datetime
import calendar


# Machine Learning
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_absolute_error,mean_squared_error,mean_squared_log_error

from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
import xgboost as xgb
from xgboost.sklearn import XGBRegressor


# Time Series Models
import pmdarima as pm
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.arima.model import ARIMA


# Statistics
import scipy.stats
import warnings
from tqdm import tqdm

# Set Options
pd.set_option('display.max_rows', 800)
pd.set_option('display.max_columns', 500)
pd.set_option('expand_frame_repr', False)
# %matplotlib inline
warnings.filterwarnings("ignore")

"""## **1. Load Dataset**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>Load Dataset</h2>
</div>

The goal is to:
- Load the datasets

Load the datasets using pd.read_csv()
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Load the datasets
# # data_path = '../Data/'
# data_path = 'Data/'  # colab
# 
# air_store_info    = pd.read_csv('air_store_info.csv')
# hpg_store_info    = pd.read_csv('hpg_store_info.csv')
# sample_submission = pd.read_csv('sample_submission.csv')
# date_info         = pd.read_csv('date_info.csv')
# hpg_reserve       = pd.read_csv('hpg_reserve.csv')
# air_visit_data    = pd.read_csv('air_visit_data.csv')
# air_reserve       = pd.read_csv('air_reserve.csv')
# store_id_relation = pd.read_csv('store_id_relation.csv')

"""### __Data Description__
This is a relational dataset from two systems. Each file is prefaced with the source (either air_ or hpg_) to indicate its origin.

Each restaurant has a unique __air_store_id__ and __hpg_store_id__. Note that not all restaurants are covered by both systems, and that you have been provided data beyond the restaurants for which you must forecast.

Latitudes and Longitudes are not exact to discourage de-identification of restaurants.

### __Air Store Info__

This file contains location and genre information about select air restaurants.
"""

air_store_info.head()

"""#### Data Description
This file contains information about select air restaurants. Column names and contents are self-explanatory.

- air_store_id - the restaurant's id in the air system
- air_genre_name
- air_area_name
- latitude
- longitude

Note: latitude and longitude are the latitude and longitude of the area to which the store belongs

### HPG Store Info

This file contains information about select hpg restaurants.
"""

hpg_store_info.head()

"""#### Data Description
This file contains information about select hpg restaurants. Column names and contents are self-explanatory.

- hpg_store_id
- hpg_genre_name
- hpg_area_name
- latitude
- longitude

Note: latitude and longitude are the latitude and longitude of the area to which the store belongs

### Sample Submission

This file shows the format in which you must forecast, including the days for which you must forecast.
"""

sample_submission.head()

"""#### Data Description
This file shows a submission in the correct format, including the days for which you must forecast.

- `id` - the id is formed by concatenating the `air_store_id` and `visit_date` with an underscore
- `visitors`- the number of visitors forecasted for the store and date combination

### Date and holiday Info

This file gives basic information about the calendar dates, day_of_week and holidays.
"""

date_info.head()

"""#### Data Description
This file gives basic information about the calendar dates in the dataset.
- calendar_date
- day_of_week
- holiday_flg - is the day a holiday in Japan

### HPG Reserve

This file contains reservations made in the hpg system.
"""

hpg_reserve.head()

"""#### Data Description
This file contains reservations made in the hpg system.

- hpg_store_id - the restaurant's id in the hpg system
- visit_datetime - the time of the visit at reservation
- reserve_datetime - the time the reservation was made
- reserve_visitors - the number of visitors for that reservation

### Air Visit Data

This file contains historical visit data for the air restaurants.
"""

air_visit_data.head()

"""#### Data Description
This file contains historical visit data for the air restaurants.

- air_store_id
- visit_date - the date
- visitors - the number of visitors to the restaurant on the date

### Air Reserve Data

his file contains reservations made in the air system. Note that the reserve_datetime indicates the time when the reservation was created, whereas the visit_datetime is the time in the future where the visit will occur.
"""

air_reserve.head()

"""#### Data Description
This file contains reservations made in the air system. Note that the reserve_datetime indicates the time when the reservation was created, whereas the visit_datetime is the time in the future where the visit will occur.

- air_store_id - the restaurant's id in the air system
- visit_datetime - the time of the reservation
- reserve_datetime - the time the reservation was made
- reserve_visitors - the number of visitors for that reservation

### Store Id Relation Data

This file allows you to join select restaurants that have both the air and hpg system.
"""

store_id_relation.head()

"""#### Data Description
This file allows you to join select restaurants that have both the air and hpg system.

- hpg_store_id
- air_store_id

## **2. Understand Data**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>Understand Data</h2>
</div>

Before attempting to solve the problem, it's very important to have a good understanding of data.

The goal of this section is to:
- Get shape and summary of data frames
- Get various statistics of data
- Get inferences or action items for next step
"""

def understand_df(df):

    # Dimensions of dataset
    print("Dimension of the dataset is", df.shape , "\n\n")

    # Head of dataset
    print("First 5 rows of the dataset are \n", df.head(), "\n\n")

    # Summary of dataset
    print("Summary of the dataset is \n", df.describe(), "\n\n")

    # Stats of dataset
    stats = []
    for col in df.columns:
        stats.append((col, df[col].nunique(), df[col].isnull().sum() * 100 / df.shape[0], df[col].value_counts(normalize=True, dropna=False).values[0] * 100, df[col].dtype))

    stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of missing values', 'Percentage of values in the biggest category', 'type'])
    print("Statistics of the dataset are \n",stats_df.sort_values('Percentage of missing values', ascending=False), "\n\n")

"""### Store Info Data - Air & HPG"""

# Air Store Info
understand_df(air_store_info)

# HPG Store Info
understand_df(hpg_store_info)

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- 829 stores in air data
- 4690 stores in hpg data
- Missing values are not there in any data
- Both the dataset have 5 columns - id,genre_name,area_name_latitude,longitude
- Area name follows a specific structure, so new features can be extracted

### Visit Data
"""

# Air Visit Data
understand_df(air_visit_data)

# Max and Min dates
mi = air_visit_data.visit_date.min()
ma = air_visit_data.visit_date.max()
print(mi, ma)
print("# Days", (pd.to_datetime(ma)-pd.to_datetime(mi)).days)

# Stores present in air visit data not present in air store info
np.setdiff1d(air_visit_data["air_store_id"], air_store_info["air_store_id"])

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- 829 stores are present.
- 252k rows present
- No Missing values
- Data collection interval: 477 days
- This is the main training data
- All the stores in visit data have corresponding info in air store info data

### Reservation Data
"""

# Air Reserve Data
understand_df(air_reserve)

# Max and Min dates: visit_datetime
mi = air_reserve.visit_datetime.min()
ma = air_reserve.visit_datetime.max()
print(mi, ma)
print("# Days", (pd.to_datetime(ma)-pd.to_datetime(mi)).days)

# Max and Min dates: reserve_datetime
mi = air_reserve.reserve_datetime.min()
ma = air_reserve.reserve_datetime.max()
print(mi, ma)
print("# Days", (pd.to_datetime(ma)-pd.to_datetime(mi)).days)

# Stores present in air reserve data not present in air store info
np.setdiff1d(air_reserve["air_store_id"], air_store_info["air_store_id"])

# HPG Reserve Data
understand_df(hpg_reserve)

# Stores present in air reserve data not present in air store info
print(np.setdiff1d(hpg_reserve["hpg_store_id"], hpg_store_info["hpg_store_id"]))
print(len(np.setdiff1d(hpg_reserve["hpg_store_id"], hpg_store_info["hpg_store_id"])))

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- 314 stores in air data (unique air_store_ids)
- 13325 stores in hpg data
- Missing values are not there in any data
- Both the dataset have 5 columns - id,genre_name,area_name_latitude,longitude
- All the stores in air reserve data have store information in air store info data
- HPG reserve data has more stores compared to hpg store info data. 8635 stores in hpg reserve data doesn't have store information in hpg store info data

### Date Info
"""

# Air Store Info Data
understand_df(date_info)

# Start Date
print(min(date_info.calendar_date))

# End Date
print(max(date_info.calendar_date))

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- Date info is present for 1 year and 5 months. The start date is 2016-01-01, the end date is 2018-05-31
- Holiday flag is also present which states if a particular day was holiday or not
- No Missing values

### Store ID relation
"""

# Air Store Info Data
understand_df(store_id_relation)

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- Only 150 stores have a mapping of air ID and hpg ID

### Submission Data
"""

# Sample Submission
understand_df(sample_submission)

# sample_submission['date'] = sample_submission.id.split("_")[2]
sample_submission['visit_date'] = sample_submission.apply(lambda x: x['id'].split("_")[2],axis=1)
sample_submission['store_type'] = sample_submission.apply(lambda x: x['id'].split("_")[0],axis=1)
sample_submission['air_store_id'] = sample_submission.apply(lambda x: x['id'].split("_")[0] + "_" + x['id'].split("_")[1] ,axis=1)
sample_submission.head()

# Sample Submission
understand_df(sample_submission)

# Start Date
print(min(sample_submission.visit_date))

# End Date
print(max(sample_submission.visit_date))

# Stores present in air reserve data not present in air store info
print(np.setdiff1d(sample_submission["air_store_id"], air_store_info["air_store_id"]))

# Stores present in air reserve data not present in air store info
print(np.setdiff1d(sample_submission["air_store_id"], air_visit_data["air_store_id"]))

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- All the predictions needs to be made for air stores
- Predictions needs to be made for 821 stores for 39 days each
- All the stores are present in air store info data and air visit data
- Since predictions needs to be made for air stores, air stores data should not be lost while merging the dataset

## **3. Data Pre-processing for EDA**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>Data Preprocessing for EDA</h2>
</div>

The goal of this section is to:
- Merge stores data (air & hpg) to get common information
- Merge reserves data to get common reservation information
- Engineer holiday related features
- Merge all the datasets to create one mega dataset for training


Let's start with the first task to merge datasets to form one.

### Prepare Store Info

Since all stores we want to predict are in `air_store`, it is safe to start from it and do left joins.

First, add `hpg_store_id` to the `air_store` table
"""

# Merge store info and store id relation dataframes
stores_df = pd.merge(air_store_info, store_id_relation, how='left', on='air_store_id')
stores_df.shape

"""How many stores didn't find a matching `hpg_store_id`?"""

# Check null values for hpg store id
stores_df['hpg_store_id'].isnull().sum()/stores_df.shape[0]

stores_df.head()

"""Add `hpg_store` information:"""

# Merge stores_df and hpg_store_info dataframes
stores_df = pd.merge(stores_df, hpg_store_info, how='left', on='hpg_store_id', suffixes=['_air', '_hpg'])

stores_df.head()

"""How many stores found no information in the `hpg_store` table?"""

# Check null values for hpg store id
stores_df['hpg_area_name'].isnull().sum()/stores_df.shape[0]

"""### Prepare Reservation Data

Take the hpg reservation data, map it to air_id and append it to air reservation data. This is done so we can leverage all of the reservation data at hand.
"""

print(hpg_reserve.shape)
hpg_reserve.head()

# Get all the reservation data of hpg stores  which have corresponding air id
hpg_reserve = pd.merge(hpg_reserve, store_id_relation, on='hpg_store_id')[air_reserve.columns]
print(hpg_reserve.shape)
hpg_reserve.head()

# Concat all the reservation data
reserves_df = pd.concat([air_reserve, hpg_reserve], axis=0)
reserves_df.shape

"""### Prepare Air Visits Data

Create 'id' column in air_visits data.
"""

air_visit_data['id'] = air_visit_data['air_store_id'].str.cat(air_visit_data['visit_date'].astype(str), sep='_')
air_visit_data = air_visit_data[["id", "air_store_id", "visit_date",  "visitors"]]
air_visit_data.head()

"""Submission data"""

sample_submission = sample_submission[["id","air_store_id", "visit_date",  "visitors"]]
sample_submission.head()

air_visit_data.shape

"""252108 records are present in air_visit data. Of that, keep only those store ids present in submission data."""

air_visit_data = air_visit_data.loc[air_visit_data.air_store_id.isin(sample_submission.air_store_id),]

"""Concat rowwise, the air visits data and the submissions data, keeping only those air ids that are present in the submission data. Because, we care to forecast only those.

Add a marker column to indicate if it a given row is a past data or future data.
"""

visits_df = pd.concat([air_visit_data, sample_submission],
                 axis=0, keys=['past','future'], names=['dataset'])\
        .reset_index(level='dataset')\
        .reset_index(drop=True)
visits_df['visit_date'] =  pd.to_datetime(visits_df['visit_date'], format='%Y-%m-%d')
print(visits_df.shape)

visits_df.head()

date_info.head()

# put as date
date_info.rename(columns={'calendar_date': 'visit_date'}, inplace=True)
date_info['visit_date'] =  pd.to_datetime(date_info['visit_date'], format='%Y-%m-%d')

"""Create 'weekend' variable."""

date_info['weekend'] = [1 if (day == 'Saturday' or day == 'Sunday') else 0 for day in date_info.day_of_week]

"""Create day_off flag"""

date_info['day_off_flg'] = 0
for i in range(date_info.shape[0]):
    weekend = date_info['weekend'][i]
    holiday = date_info['holiday_flg'][i]
    if weekend == 1 or holiday == 1:
        date_info['day_off_flg'][i] = 1
    else:
        date_info['day_off_flg'][i] = 0

# Extract features: Tomorrow is holiday or yesterday is holiday
date_info['tomorrow_is_holiday']  = date_info.day_off_flg.shift(-1).fillna(0).astype(int)
date_info['yesterday_is_holiday'] = date_info.day_off_flg.shift(1).fillna(0).astype(int)

date_info.head()

"""feature idea: yesterday is holiday but today is not."""

visits_df = pd.merge(visits_df, date_info, on='visit_date')
visits_df.head()

"""This is our primary dataset.

<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

We have 3 datasets now, we will use these for EDA:
1. Stores Information: **stores_df**
2. Reservations Information: **reserves_df**
3. Visits Information: **visits_df**

Let's create master data frame. It will be used while modelling, but in order to do EDA we will use above 3 files

## **4. Exploratory Data Analysis**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>Exploratory Data Analysis</h2>
</div>


Exploratory data analysis is an approach to analyze or investigate data sets to find out patterns and see if any of the variables can be useful in predecting the y variables. Visual methods are often used to summarise the data. Primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing tasks.

The goal of this section is to:
- Get insights from the visitors data
- Check the distribution of target variable and decide if it needs to be transformed
- Analyze the reservations and find out relation between reservation and visits
- Get insights from the stores info data
- Extract the significant variables (will be covered in next section)

### Visitors

#### Visitors each day
"""

visits_df_past = visits_df[visits_df.dataset == "past"]
visits_df_past['visit_date'] =  pd.to_datetime(visits_df_past['visit_date'], format='%Y-%m-%d')

# Visitor each day
plt1 = visits_df_past.groupby(['visit_date'], as_index=False).agg({'visitors': np.sum})
plt1=plt1.set_index('visit_date')
plt1.plot(figsize=(15, 6))
plt.ylabel("Sum of Visitors")
plt.title("Visitor each day")

visits_df["jump_flag"] = visits_df["visit_date"].apply(lambda x: '1' if x.strftime('%Y-%m-%d') < '2016-7-01' else 0)

"""There is a sudden jump in the daily visitors from July onwards. Could be because new restaurants might be added. Let's check."""

# Number of store_ids by month
plt2 = visits_df_past.groupby(['visit_date'], as_index=False).agg({'air_store_id': ['count', 'size', 'nunique']})
plt2

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- There is an interesting long-term step structure in the overall time series. This might be related to new restaurants being added to the data base. In addition, we already see a periodic pattern that most likely corresponds to a weekly cycle.

#### Visitors Distribution
"""

# plot
fig, ax = plt.subplots(1,2)
fig.set_size_inches(15, 4, forward=True)

sns.distplot(visits_df_past['visitors'], kde = False, color ='red', bins = 400, ax=ax[0])
sns.distplot(np.log1p(visits_df_past['visitors']), kde = False, color ='red', bins = 30, ax=ax[1])

ax[0].set_xlabel('Visitors')
ax[0].set_ylabel('Frequency')
ax[1].set_xlabel('Log of Visitors')
ax[1].set_ylabel('Frequency')

fig.suptitle('Distribution of visitors', fontsize=16)

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- The number of visitors per visit per restaurant per day peaks at around 20 (the top line in left chart). The distribution extends up to 100 and, in rare cases, beyond.
- The distribution of visitors is skewed
- Log transformation can be used to make it closer to shape of normal distribution.

**Median number of visitors in day of a week and month of a year.**

Prepare data.
"""

# Median number of visitor in day of a week
plt1 = visits_df_past.groupby(['day_of_week'], as_index=False).agg({'visitors': np.median})
days = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']
mapping = {day: i for i, day in enumerate(days)}
key = plt1['day_of_week'].map(mapping)
plt1 = plt1.iloc[key.argsort()].set_index('day_of_week').reset_index()


# Median number of visitor in Month of a Year
visits_df_past['Month']=visits_df_past['visit_date'].apply(lambda x: calendar.month_name[x.month])
plt2 = visits_df_past.groupby(['Month'], as_index=False).agg({'visitors': np.median})

Months = ['January','February','March','April','May','June','July','August','September','October','November','December']
mapping = {Month: i for i, Month in enumerate(Months)}
key = plt2['Month'].map(mapping)
plt2 = plt2.iloc[key.argsort()].set_index('Month').reset_index()

"""Plot it."""

#plot
fig, ax =plt.subplots(1,2)
fig.set_size_inches(15,4, forward=True)

sns.barplot(x="day_of_week",y="visitors",data=plt1,ax=ax[0])
sns.barplot(x="Month",y="visitors",data=plt2,ax=ax[1])
ax[0].set_xlabel('Day of week')
ax[0].set_ylabel('Median Visitors')
ax[1].set_ylabel('Median Visitors')
for ax in ax:
    for label in ax.get_xticklabels():
        label.set_rotation(45)

fig.suptitle('Distribution of visitors', fontsize=16)

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- Friday and the weekend appear to be the most popular days, which is expected.
- Monday and Tuesday have the lowest numbers of average visitors.
- There is a certain amount of variation during the year.
- December appears to be the most popular month for restaurant visits.
- The period of March - May is also consistently busy.
- The period of August - September appears to receive lesser visitors relatively.

### Reservation
"""

reserves_df.head()

"""Reserve new date related variables."""

reserves_df['visit_datetime'] =  pd.to_datetime(reserves_df['visit_datetime'], format='%Y-%m-%d %H:%M:%S')
reserves_df['reserve_datetime'] =  pd.to_datetime(reserves_df['reserve_datetime'], format='%Y-%m-%d %H:%M:%S')
reserves_df['visit_hour'] = reserves_df['visit_datetime'].apply(lambda x: x.time().hour)
reserves_df['reserve_hour'] = reserves_df['reserve_datetime'].apply(lambda x: x.time().hour)
reserves_df['visit_date'] = reserves_df['visit_datetime'].apply(lambda x: x.date())
reserves_df['reserve_date'] = reserves_df['reserve_datetime'].apply(lambda x: x.date())
reserves_df['reserve_ahead'] = reserves_df["visit_datetime"] - reserves_df["reserve_datetime"]
reserves_df['hours_ahead'] = (reserves_df["reserve_ahead"]/pd.Timedelta('1 hour')).astype(int)
reserves_df['days_ahead'] = reserves_df.reserve_ahead.apply(lambda delta_t: delta_t.days)

"""Plot"""

# Reservations each day
plt1 = reserves_df.groupby(['reserve_date'], as_index=False).agg({'reserve_visitors': np.sum})
plt1 = plt1.set_index('reserve_date')
plt1.plot(figsize=(15, 6))
plt.ylabel("Sum of Reserve Visitors")
plt.xlabel("Reservation Date")
plt.title("Reserve Visitor each day")

"""Number of visitors increased from 1st July 2016 as per visitors data as more restaurants were added to the data. But, the same isn't observed in reservation data. Let's see the restaurant count for reservation data"""

# Number of store_ids by month
plt2 = reserves_df.groupby(['reserve_date'], as_index=False).agg({'air_store_id': ['count', 'size', 'nunique']})
plt2

"""New restaurants were added to the reservation data only after 24th Oct 2016, that's why the upward trend of visitors didd't match the trend of reservations.

<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- Reservations made in 2016 are in the range of 100 to 1000 for the first 10 months of the year and increased to the range of 1500 to 3500 towards the end of the year
- New restaurants were added to the data from 24th Oct 2016
- Number of reservations stayed strong during 2017, in the range of 1500 to 3000
- A clear seasonality can be seen in reservations, probably weekly seasonality
"""

# Reservations each day
plt1 = reserves_df.groupby(['visit_hour'], as_index=False).agg({'reserve_visitors': np.sum})
plt2 = reserves_df.groupby(['reserve_hour'], as_index=False).agg({'reserve_visitors': np.sum})

#plot
fig, ax =plt.subplots(1,2)
fig.set_size_inches(15,4, forward=True)

sns.barplot(x="visit_hour",y="reserve_visitors",data=plt1,ax=ax[0])
sns.barplot(x="reserve_hour",y="reserve_visitors",data=plt2,ax=ax[1])
ax[0].set_xlabel('Visit Hours')
ax[1].set_xlabel('Reserve Hours')
ax[0].set_ylabel('Reservations')
ax[1].set_ylabel('Reservations')
for ax in ax:
    for label in ax.get_xticklabels():
        label.set_rotation(45)

fig.suptitle('Distribution of visits and reservations', fontsize=16)

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- Reservations are made typically for the dinner hours in the evening and evening
- Majority of reservations are made during evening hours 5pm to 7pm
"""

# Reservations each day
plt1 = reserves_df.groupby(['hours_ahead'], as_index=False).agg({'reserve_visitors': np.sum})

#plot
plt.figure(figsize=(12,7))
sns.barplot(x="hours_ahead",y="reserve_visitors",data=plt1)

plt.xticks(np.arange(0, 100, 4),
    rotation=45
);

"""Nothing can be seen clearly, let's filter the data for hours_ahead less than 100"""

#plot
plt.figure(figsize=(12,7))
sns.barplot(x="hours_ahead",y="reserve_visitors",data=plt1[(plt1['hours_ahead'] <= 100)])

plt.xticks(np.arange(0, 100, 4),
    rotation=45
);

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- The time between making a reservation and visiting the restaurant follow a nice 24 hours pattern.
- The most popular strategy is to reserve a couple of hours before the visit, but if the reservation is made more in advance then it seems to be common to book a table in the evening for one of the next evenings.
- There are people who book well in advance even more than couple of months, might be coroporate bookings or pre planned parties

### Store Info Data
"""

stores_df.head()

!pip install folium

# Location of stores in Japan
import folium
from folium import plugins

location =stores_df.groupby(['latitude_air', 'longitude_air']).size().reset_index()
locationheat = location[['latitude_air', 'longitude_air']]
locationheat = locationheat.values.tolist()

map1 = folium.Map(location=[39, 139],
                  tiles = "Stamen Watercolor",
                  zoom_start = 5)

heatmap=plugins.HeatMap(locationheat).add_to(map1)
map1

# Number of restaurants in area: Air Data
airS1=stores_df['air_area_name'].value_counts().reset_index().sort_index()
airS2=stores_df['air_genre_name'].value_counts().reset_index().sort_index()

fig,ax = plt.subplots(1,2)
sns.barplot(y='index' ,x='air_area_name',data=airS1.iloc[:15],ax=ax[0])
sns.barplot(y='index' ,x='air_genre_name',data=airS2.iloc[:15],ax=ax[1])
fig.set_size_inches(25,12)
ax[0].set_ylabel('Number of Restaurants')
ax[1].set_ylabel('Number of Restaurants')

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- Fukuoka-ken Fukuoka-shi Daimyo has the maximum restaurants
- Izakaya genre has the maximum resturants

## **5. Feature Engineering**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>Feature Engineering</h2>
</div>

Feature engineering is the process of using domain and statistical knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself.

### Domain Specific Features

You need to engineer the domain specific features. This might boost up the predictive power. This often gives better performing models

Domain knowledge is one of the key pillars of data science. So always understand the domain before attempting the problem.

### Visits Data
"""

visits_df.head()

"""#### Year, Day, Month etc

- Engineer features like weekday, year, month, day of year, days in month, week of year, is it month end
"""

# Extract features
visits_df['weekday']       = visits_df.visit_date.dt.dayofweek
visits_df['year']          = visits_df.visit_date.dt.year
visits_df['month']         = visits_df.visit_date.dt.month
visits_df['day_of_year']   = visits_df.visit_date.dt.dayofyear
visits_df['days_in_month'] = visits_df.visit_date.dt.days_in_month
visits_df['week_of_year']  = visits_df.visit_date.dt.weekofyear
visits_df['is_month_end']  = visits_df.visit_date.dt.is_month_end

"""#### Holiday Features
- Extract holiday features
"""

# Extract features
visits_df['tomorrow_is_holiday']  = visits_df.day_off_flg.shift(-1).fillna(0).astype(int)
visits_df['yesterday_is_holiday'] = visits_df.day_off_flg.shift(1).fillna(0).astype(int)

"""#### Stores Information"""

stores_df.head()

"""__Split area_name string to get different levels of location names__"""

# Extract features from air_area_name
area_split              = stores_df.air_area_name.str.split(' ', expand=True)

stores_df['Todofuken']  = area_split[0]  # prefecture
stores_df['city']       = area_split[1]
stores_df['street']     = area_split.iloc[:, 2:].apply(lambda row: ' '.join(row.dropna()), axis=1)

stores_df.head()

"""__Count the number of stores in the same location__"""

# Extract Features
n_stores_by_street    = stores_df.groupby(['air_area_name']).size().to_frame(name='n_stores_same_street').reset_index()
n_stores_by_city      = stores_df.groupby(['Todofuken', 'city']).size().to_frame(name='n_stores_same_city').reset_index()
n_stores_by_Todofuken = stores_df.groupby('Todofuken').size().to_frame(name='n_stores_same_Todofuken').reset_index()

# Merge the features to stores_df
stores_df = pd.merge(left=stores_df, right=n_stores_by_street, how='left', on='air_area_name')
stores_df = pd.merge(left=stores_df, right=n_stores_by_city, how='left', on=['Todofuken', 'city'])
stores_df = pd.merge(left=stores_df, right=n_stores_by_Todofuken, how='left', on='Todofuken')

stores_df.head()

"""#### Create Master Data Frame

Create a master dataframe that combines visits, stores data and reservations data.

In the process, aggregate reservations data by store_id and visit date to create the number of visitors every day as a new feature.
"""

# Convert visit date features to %Y-%m-%d format
reserves_df['visit_date'] =  pd.to_datetime(reserves_df['visit_date'], format='%Y-%m-%d')
temp = reserves_df.copy()
temp.head()

# Group the values by air store id and visit date
temp = temp.groupby(["air_store_id", "visit_date"], as_index= False).agg({'reserve_visitors': ['count','sum'], 'visit_hour': np.mean,'reserve_hour': np.mean, 'hours_ahead' : np.mean  })
temp.columns = ["air_store_id", "visit_date", "reserve_vistors_count", "reserve_visitors", "visit_hour", "reserve_hour", "hours_ahead"]
temp.head()

# Merge the datasets and create a master data set
master_df = pd.merge(visits_df,stores_df, on = "air_store_id", how = "left")
master_df = pd.merge(master_df,temp, on = ["air_store_id", "visit_date"], how = "left")
print(master_df.shape)
master_df.head()

"""### Feature Engineering: Interaction between features

Create features by combining various features

### Combination of categorical columns
- Create features by combining the categorical columns
"""

def catStrFeatures(df, colname1, colname2, sep='_'):
    series = df[colname1].astype(str).str.cat(df[colname2].astype(str), sep=sep)
    return series

# Create features by concatenating 2 or more features
master_df['area_genre'] = catStrFeatures(master_df, 'air_area_name', 'air_genre_name')
master_df['store_weekday'] = catStrFeatures(master_df, 'air_store_id', 'weekday')
master_df['store_weekday_holiday'] = catStrFeatures(master_df, 'store_weekday', 'holiday_flg')

"""### Target Variable Derived

- Find the mean, median, max, min of visitors for each store-weekday-holiday combination.
- The idea is to provide a very rough range estimate of # visitors based on historic statistics.
"""

# Get the visitors statistics like mean visitors, median visitors, min and max visitors
visitor_stats = master_df\
                .query('dataset == "past"')\
                .groupby(['air_store_id', 'weekday', 'holiday_flg'])\
                ['visitors']\
                .agg(['mean','median', 'min', 'max'])\
                .rename(columns=lambda colname: str(colname)+'_visitors')\
                .reset_index()

# Merge the visitor stats data to master dataframe
master_df = master_df.merge(visitor_stats, how='left', on=['air_store_id', 'weekday', 'holiday_flg'])

master_df.head()

"""Make a copy of the dataframe for univariate models."""

# Make a copy of master dataframe for univariate models
df_univariate = master_df.copy()

df_univariate.to_csv("df_univariate.csv",index = False)

"""## **6. Exploratory Data Analysis - Part 2**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>6. Exploratory Data Analysis - Part 2</h2>
</div>

- Distribution of holidays and impact on visitors
- Analyze the genre information

### Distribution of holiday flag

Let's check the count of holidays vs holidays
"""

# Count of holidays and non holidays
plt.figure(figsize=(8,4))
sns.countplot(x='holiday_flg', data=master_df)
plt.title("Count Plot - Holiday Flag", fontsize = 16)
plt.ylabel("Count", fontsize = 12)
plt.xlabel("Holiday Flag", fontsize = 12)
plt.show()

# Value Count of holiday flag
master_df.holiday_flg.value_counts()

"""**Distribution of visitors by holiday flag**"""

# Check distribution of visitors for holiday flag
plt.figure(figsize=(8,4), dpi= 80)
sns.boxplot(x ='holiday_flg', y = 'visitors', data = master_df, notch=False)
plt.title('Visitors by Holiday Flag', fontsize = 16)
plt.ylabel("Visitors", fontsize = 12)
plt.xlabel("Holiday Flag", fontsize = 12)
plt.ylim(0,70)
plt.show()

"""Certain holidays have lower visitors as well.

<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- The count of holidays is only 0.06 times of the count of non holidays
- The number of visitors on holidays has a larger band. However, a clear shift in number of visitors can't be seen in the visitor's distribution for holiday flag, the possible reason can be the fact that weekends aren't included in the holidays

Let's look at the distribution of visitors for all the off days
"""

# Count of holidays and non holidays
plt.figure(figsize=(8,4))
sns.countplot(x='day_off_flg', data=master_df)
plt.title("Count Plot - Day Off Flag", fontsize = 16)
plt.ylabel("Count", fontsize = 12)
plt.xlabel("Day Off Flag", fontsize = 12)
plt.show()

"""**Number of visitors by Day off Flag**"""

# Check distribution of visitors for Day-Off flag
plt.figure(figsize=(8,4), dpi= 80)
sns.boxplot(x ='day_off_flg', y = 'visitors', data = master_df, notch=False)
plt.title('Visitors by Day Off Flag', fontsize = 16)
plt.ylabel("Visitors", fontsize = 12)
plt.xlabel("Day Off Flag", fontsize = 12)
plt.ylim(0,70)
plt.show()

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- The number of visitors are higher on an off day as compared to the working day

Let's look at another way to analyze it

#### Violin Plot
"""

# Check distribution of visitors for holiday flag using violin plot
plt.figure(figsize=(8,4), dpi= 80)
sns.violinplot(x ='day_off_flg', y = 'visitors', data = master_df, scale='width', inner='quartile')
plt.title('Visitors by Day Off Flag', fontsize = 16)
plt.ylabel("Visitors", fontsize = 12)
plt.xlabel("Day Off Flag", fontsize = 12)
plt.ylim(-10,100)
plt.show()

"""### Visitors by Genre

Let's analyze various genre and find out if there's a specific genre which is dominating over others
"""

# Aggregate the visitors data for Air Genre and visit date
genre_df = master_df.loc[master_df.dataset == "past",:].groupby(['air_genre_name', 'visit_date'], as_index = False).agg({'visitors' : 'mean'})
genre_df.head()

# Draw time series plots for every Air Genre
fig, axs = plt.subplots(5,3, figsize=(25, 25))
fig.subplots_adjust(hspace = .5, wspace=.001)

axs = axs.ravel()
for i in range(len(genre_df.air_genre_name.unique())):
    df = genre_df.loc[genre_df.air_genre_name == genre_df.air_genre_name.unique()[i],:]
    axs[i].plot(df.visit_date, df.visitors)
    axs[i].set_ylim([0, 120])
    axs[i].set_title(genre_df.air_genre_name.unique()[i])

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- The mean values range between 0 and 80 visitors per genre per day
- The trend seems resonably stable for every genre. However there's an upward trend for “Creative Cuisine” and “Okonomiyaki” genre
- The trend of “Karaoke” and “Asian” are understandably more noisy as compared to other genres

Let's look at another very import plot to analyze the distribution of every class in dataframe

### Joy Plot
"""

import joypy as joypy

# Draw Joy Plot
plt.figure(figsize=(16,10), dpi= 80)
fig, axes = joypy.joyplot(genre_df, column='visitors', by="air_genre_name", figsize=(14,10))
plt.title('Joy Plot of Visitors by Air Genre', fontsize=22)
plt.show()

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- Asian, International and Karake genre are boardly distributed, impying more variance in daily values.

## 8. Statistical Significance Tests

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>Statistical Significance Test</h2>
</div>

## **7. Spearmanr Correlation  - Continuous Variables**
Since the data is not normally distributed, you should not go for Pearsonr Correlation. Rather go for Spearmanr Correlation
"""

# Get the spearmanr correlation coeffecient and p value for holiday flag
scipy.stats.spearmanr(master_df.visitors, master_df.holiday_flg, nan_policy = 'omit')

print(master_df.columns)
master_df.head()

# Get the list of numerical columns
numerical_columns = ['visitors', 'day_of_year', 'days_in_month', 'week_of_year', 'latitude_air',
       'longitude_air', 'latitude_hpg',	'longitude_hpg', 'n_stores_same_street', 'n_stores_same_city', 'n_stores_same_Todofuken',
       'reserve_vistors_count', 'reserve_visitors', 'visit_hour',
       'reserve_hour', 'hours_ahead',  'mean_visitors', 'median_visitors',
       'min_visitors', 'max_visitors']
numerical_columns

# Create a dictionary with correlation and p values for every feature
correlation = {'feature' : [], 'correlation' : [], 'p-value' : []}
for col in numerical_columns[1:]:
    correlation['feature'].append(col)
    correlation['correlation'].append(scipy.stats.spearmanr(master_df.visitors, master_df[col], nan_policy = 'omit')[0])
    correlation['p-value'].append(scipy.stats.spearmanr(master_df.visitors, master_df[col], nan_policy = 'omit')[1])

# Convert the dictionary to dataframe
correlation_df = pd.DataFrame(correlation)
correlation_df

# Significant numeric variables
significant_variables = correlation_df.loc[correlation_df["p-value"] <= 0.05,"feature"]
significant_variables

# # Let's create the correlations matrix between all the variables
# plt.figure(figsize=(30,30))
# p=sns.heatmap(master_df.corr(), annot=True,cmap='RdYlGn',center=0)

"""## **8. ANOVA**"""

categorical_columns = [i for i in master_df.columns if i not in numerical_columns]
categorical_columns

import statsmodels.api as sm
from statsmodels.formula.api import ols

"""#### ANOVA test using ols model"""

# Build OLS model on 'air_store_id' column

col = 'air_store_id'
model = ols(('visitors ~ ' + col),                 # Model formula
            data = master_df).fit()

# Get ANOVA summary
anova_result = sm.stats.anova_lm(model, typ=2)
print(anova_result)

# p-value
anova_result["PR(>F)"][0]

# # Get significant categorical variables based on ANOVA
# for col in categorical_columns[3:]:   # first 3 features are just flags and id
#     model = ols(('visitors ~ ' + col), data = master_df).fit()

#     # Get ANOVA summary
#     anova_result = sm.stats.anova_lm(model, typ=2)

#     if anova_result["PR(>F)"][0] <= 0.05:
#         print(col)

"""## **9. Feature Encoding**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>9. Feature Encoding</h2>
</div>

Encoding is the process of converting data from one form to another. Most of the Machine learning algorithms can not handle categorical values unless we convert them to numerical values. Many algorithm’s performances vary based on how Categorical columns are encoded.
- **Label encoding** - Label Encoding refers to converting the labels into numeric form so as to convert it into the machine-readable form. Machine learning algorithms can then decide in a better way on how those labels must be operated. It is an important pre-processing step for the structured dataset in supervised learning.

It is a popular encoding technique for handling categorical variables. In this technique, each label is assigned a unique integer based on alphabetical ordering.
"""

categorical_columns = [i for i in master_df.columns if i not in numerical_columns]
categorical_columns

categorical_columns.remove("dataset")
categorical_columns.remove("id")
categorical_columns.remove("visit_date")
# categorical_columns.remove("air_store_id")

master_df.head()

# Label encode the variables
for col in categorical_columns:
    lbl = LabelEncoder()
    lbl.fit(list(master_df[col].values))
    master_df[col] = lbl.transform(list(master_df[col].values))

master_df.head()

"""## **10. Data Preprocessing for Model Building**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>10. Data Preprocessing for model building</h2>
</div>



The goal of this section is to:
- Drop the column which may not be useful
- Split the dataset in training and test sets


**Tip : Save the train df, and clean all memory**
"""

# Save master df to csv file
# master_df.to_csv("training_df.csv",index = False)

# Read master df
master_df = pd.read_csv("training_df.csv")

# Split the past data and future data
train,future = master_df[master_df['dataset'] == 'past'],master_df[master_df['dataset'] == 'future']

"""Split the historical data and future dataset"""

train.head()

# Drop the dataset and id columns
train = train.drop(['dataset', 'id',],axis=1)

"""#### Split the dataset for training and testing"""

# Print min and max visit date
print(train.visit_date.min())
print(train.visit_date.max())

train.head()

# Get the train dataset, we will use the data till end of Jan 2017
train_selector = train.visit_date < '2017-02-01'
X_train = train[train_selector]
y_train = train[train_selector]['visitors'].apply(np.log1p)    # apply np.log1p() (log(1+x)) to visitors count, to correct for high skewness
# y_train = train[train_selector]['visitors']    # apply np.log1p() (log(1+x)) to visitors count, to correct for high skewness

print('Training dataset dimensions')
print('- X_train:', X_train.shape)
print('- y_train:', y_train.shape)

X_train.head()

# Convert it back to original scale and check
y_original = y_train.apply(np.exp) - 1
print(y_original.head())
print(train[train_selector]['visitors'].head())

X_train.head()

y_train.head()

# Get the test dataset, we will use the data from start of Feb 2017
test_selector = train.visit_date >= '2017-02-01'
X_test = train[test_selector]
y_test = train[test_selector]['visitors'].apply(np.log1p)
# y_test = train[test_selector]['visitors']
print('Testing dataset dimensions')
print('- X_test:', X_test.shape)
print('- y_test:', y_test.shape)

# Proportion of training and test dataset
X_train.shape[0]/X_test.shape[0]

"""Train dataset is 3.35 times of test dataset, so it's a good mix"""

# Drop the visitors and visit date column from train and test X dataset
# X_train = X_train.drop(["visit_date","visitors"],axis=1)
# X_test = X_test.drop(["visit_date","visitors"],axis=1)

X_train = X_train.drop(["visitors"],axis=1)
X_test = X_test.drop(["visitors"],axis=1)

# Head of X_train
X_train.head()

"""## **11. Evaluation Metrics for Regression**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Model Building Starts Here</h3>
</div>

.
.
.
.



<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>11. Evaluation Metrices - Regression</h2>
</div>

- Mean Absolute Error (MAE)

- Mean Absolute Percentage Error (MAPE)

- Root Mean Squared Log Error (RMSLE)  -- Final Evaluation Metric for this problem

The Root Mean Squared Log Error (RMSLE) can be defined using a slight modification on sklearn's mean_squared_log_error function, which itself a modification on the familiar Mean Squared Error (MSE) metric.

The formula for RMSLE is represented as follows:

RMSLE formula![image.png](attachment:image.png)

Where:

𝑛  is the total number of observations in the (public/private) data set,

𝑝𝑖  is your prediction of target, and

𝑎𝑖  is the actual target for  𝑖 .

𝑙𝑜𝑔(𝑥)  is the natural logarithm of  𝑥  ( 𝑙𝑜𝑔𝑒(𝑥) .
"""

def RMSLE(y_true, y_pred):
    return np.sqrt(mean_squared_log_error(y_true, y_pred))

def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

# import
df_univariate = pd.read_csv("df_univariate.csv")
df_univariate.head()

"""## **Time Series Forecasting - ARIMA**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>Time Series Forecasting</h2>
</div>

Time series is a sequence of observations recorded at regular time intervals.

Depending on the frequency of observations, a time series may typically be hourly, daily, weekly, monthly, quarterly and annual. Sometimes, you might have seconds and minute-wise time series as well, like, number of clicks and user visits every minute etc.
In our case the time series can be weekly or monthly or yearly. By looking at the data, weekly time series makes more sense

##  ARIMA Forecasting for Each Restaurant

ARIMA, short for ‘Auto Regressive Integrated Moving Average’ is actually a class of models that ‘explains’ a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values.
"""

print("Earliest Date :", df_univariate.visit_date.min())
print("Latest Date   :", df_univariate.visit_date.max())

"""We will exclude submission data and take past data alone and split it into training and test datasets."""

# Filter the historical data
df_tsf = df_univariate.loc[df_univariate.dataset == "past",]
df_tsf.head()

"""<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>12a. ARIMA Forecasting using Statsmodels</h2>
</div>
"""

from statsmodels.tsa.arima.model import ARIMA

"""**Get unique restaurant IDs**"""

df_tsf.head()

air_store_id = df_tsf.air_store_id.unique()

"""**Create Train and Test Data**"""

# CREATE TRAIN AND TEST DATA FOR PARTICULAR RESTAURANT
df_tsf_restaurant = df_tsf.loc[df_tsf['air_store_id']==air_store_id[10], :]

# Get the train dataset, we will use the data till end of Jan 2017 for training
train_selector = df_tsf_restaurant.visit_date < '2017-02-01'
train = df_tsf_restaurant[train_selector]

test_selector = df_tsf_restaurant.visit_date >= '2017-02-01'
test = df_tsf_restaurant[test_selector]

"""**View**"""

df_tsf_restaurant.reset_index(drop=True, inplace=True)
df_tsf_restaurant.head()

"""**Create Time Series**"""

ts = train.loc[:, ['visit_date', 'visitors']]
ts['visit_date'] = pd.to_datetime(ts['visit_date'])
ts.set_index('visit_date', inplace=True)
ts.head()

"""**Plot**"""

# Plot
ts.plot(figsize=(12, 7), title=df_tsf_restaurant.air_store_id.values[0]);

"""**ADF Test for Stationarity**

The Augmented Dickey Fuller Test tests for stationarity
"""

from statsmodels.tsa.stattools import adfuller
result = adfuller(ts.dropna())
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print(f'Inference: The time series is {"non-" if result[1] >= 0.05 else ""}stationary')

"""**Check First order differenced series**"""

# First Order Differenced
from statsmodels.tsa.stattools import adfuller
result = adfuller(ts.diff().dropna())
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print(f'Inference: The time series is {"non-" if result[1] >= 0.05 else ""}stationary')

"""### Determining P and Q

**ACF and PACF Plots**
"""

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# PACF plot of 1st differenced series
plt.rcParams.update({'figure.figsize':(7, 4), 'figure.dpi':120})
plot_pacf(ts.diff().dropna());

# ACF plot of 1st differenced series
plot_acf(ts.diff().dropna());

"""Train the ARIMA model

"""

# Orders to try
order = (5, 1, 2)
order = (4, 1, 2)


# Train model
model = ARIMA(endog=ts, order=order)
fit = model.fit()

fit.summary()

# Get prediction for test duration
predictions = pd.Series(fit.forecast(len(test)))
predictions = predictions.map(lambda x: x if x >= 0 else 0)
print(predictions.head())
actuals = test['visitors']

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(predictions, actuals))
print("\n RMSLE : \n", RMSLE(predictions, actuals))
print("\n MAPE : \n", mean_absolute_percentage_error(predictions, actuals))

"""**Plot Actual vs Predicted**"""

# plot predictions and actual values
plt.figure(figsize=(10,5))
plt.plot(predictions.values, label='Actuals');
plt.plot(actuals.values, label='Predicteds');
plt.legend(loc='best');

"""### **Durbin Watson Statistic & Check for Pattern in Errors**

Line Plot
"""

errors = (actuals.values-predictions.values)
plt.plot(errors);

"""**Check Auto Correlation**"""

plot_acf(errors);

"""**Durbin Watson Statistic**

DW statistic is used to check for serial correlation.

When there is no autocorrelation, the value of the statistic equals 2. When closer to 0 -> implies positive serial correlation. When closer to 4 -> implies negative serial correlation.

The test statistic is approximately equal to 2*(1-r) where r is the sample autocorrelation of the residuals.

![DW.PNG](data:image/png;base64,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)
"""

from statsmodels.stats.stattools import durbin_watson
durbin_watson(errors)













"""## **12b. Auto ARIMA Modeled at Restaurant Level Time Series**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>12b. Auto ARIMA Modeled at Restaurant Level Time Series</h2>
</div>


"""

air_store_id = df_tsf.air_store_id.unique()
air_store_id[:10]

# CREATE TRAIN AND TEST DATA FOR PARTICULAR RESTAURANT
df_tsf_restaurant = df_tsf.loc[df_tsf['air_store_id']==air_store_id[10], :]


# Get the train dataset, we will use the data till end of Jan 2017 for training
train_selector = df_tsf_restaurant.visit_date < '2017-02-01'
train = df_tsf_restaurant[train_selector]


test_selector = df_tsf_restaurant.visit_date >= '2017-02-01'
test = df_tsf_restaurant[test_selector]

df_tsf_restaurant.reset_index(drop=True, inplace=True)
df_tsf_restaurant.head()

# Plot
df_tsf_restaurant.visitors.plot(figsize=(12, 7));

# Fit auto_arima on train set
model = pm.auto_arima(train.visitors, start_p = 1, start_q = 1,
                          max_p = 7, max_q = 3, m = 30,
                          seasonal = False,
                          d = None, trace = True,
                          error_action ='ignore',
                          suppress_warnings = True,
                          stepwise = True)

# To print the summary
model.summary()

"""#### Predict"""

# Get prediction for test duration
predictions = pd.Series(model.predict(len(test)))
predictions = predictions.map(lambda x: x if x >= 0 else 0)

actuals = test['visitors'].reset_index(drop = True)

"""#### Evaluate"""

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(predictions, actuals))
print("\n RMSLE : \n", RMSLE(predictions, actuals))
print("\n MAPE : \n", mean_absolute_percentage_error(predictions, actuals))

"""#### Plot"""

# plot predictions and actual values
predictions.plot(legend = True, label = "Prediction", xlabel = "Index", ylabel = "Visitors",  figsize=(10, 7))
actuals.plot(legend = True, label = "Actual");

"""### Forecast and Evaluate for Multiple Restaurants"""

# CREATE TRAIN AND TEST DATA FOR PARTICULAR RESTAURANT
mapes, rmsles, maes = [], [], []

for i in range(12):
    print("==============================================")
    print(air_store_id[i]);
    df_tsf_restaurant = df_tsf.loc[df_tsf['air_store_id']==air_store_id[i], :]

    # Get the train dataset, we will use the data till end of Jan 2017 for training
    train_selector = df_tsf_restaurant.visit_date < '2017-02-01'
    train = df_tsf_restaurant[train_selector]

    test_selector = df_tsf_restaurant.visit_date >= '2017-02-01'
    test = df_tsf_restaurant[test_selector]

    # Fit auto_arima on train set
    model = pm.auto_arima(train.visitors, start_p = 1, start_q = 1,
                            max_p = 7, max_q = 3, m = 30,
                            seasonal = False,
                            d = None, trace = True,
                            error_action ='ignore',
                            suppress_warnings = True,
                            stepwise = True)

    # To print the summary
    model.summary()


    # Get prediction for test duration
    predictions = pd.Series(model.predict(len(test)))
    predictions = predictions.map(lambda x: x if x >= 0 else 0)

    actuals = test['visitors'].reset_index(drop = True)

    # Evaluation Metric
    mape  = mean_absolute_percentage_error(predictions, actuals)
    rmsle = RMSLE(predictions, actuals)
    mae = mean_absolute_error(predictions, actuals)

    print("\n MAPE : \n", mean_absolute_percentage_error(predictions, actuals))
    print("\n MAE : \n ", mean_absolute_error(predictions, actuals))
    print("\n RMSLE : \n", RMSLE(predictions, actuals))

    mapes.append(round(mape, 2))
    rmsles.append(rmsle)
    maes.append(mae)

    # plot predictions and actual values
    predictions.plot(legend = True, label = "Prediction", xlabel = "Index", ylabel = "Visitors",  figsize=(10, 5))
    actuals.plot(legend = True, label = "Actual");
    plt.title(f"MAPE: {mape}")
    plt.show()

"""#### Errors

**Replace Inf with NaN, to calculate mean**
"""

mapes_clean = pd.Series(mapes).map(lambda x: np.nan if np.isinf(x) else x).values.round(2)
maes_clean = pd.Series(maes).map(lambda x: np.nan if np.isinf(x) else x).values.round(2)
rmsles_clean = pd.Series(rmsles).map(lambda x: np.nan if np.isinf(x) else x).values.round(2)

"""Calculate mean"""

print("MAPES : ", np.nanmean(mapes_clean).round(2), mapes_clean)
print("MAES  : ", np.nanmean(maes_clean).round(2), maes_clean)
print("RMSLES: ", np.nanmean(rmsles_clean).round(2), rmsles_clean)

"""Let's build time series model for Air Genre "Yakiniku/Korean food", then go to restaurant level.


<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>13. ARIMA - AIR Genre level Model</h2>
</div
"""

# Reservations each day for air genre
df_genre = df_tsf.groupby(['air_genre_name','visit_date'], as_index=False).agg({'visitors': np.sum,
                                                                                'holiday_flg': np.mean,
                                                                                'tomorrow_is_holiday': np.mean,
                                                                                'yesterday_is_holiday': np.mean,
                                                                                'weekday': np.mean,
                                                                                'month': np.mean,
                                                                                'day_of_year': np.mean,
                                                                                'days_in_month': np.mean,
                                                                                'week_of_year': np.mean,
                                                                                'is_month_end': np.mean})
df_genre = df_genre.loc[df_genre['air_genre_name'] =="Yakiniku/Korean food",]
df_genre.reset_index(drop = True, inplace = True)
df_genre.head(35)

# Get the train dataset, we will use the data till end of Jan 2017 for training
train_selector = df_genre.visit_date < '2017-02-01'
train = df_genre[train_selector]
test_selector = df_genre.visit_date >= '2017-02-01'
test = df_genre[test_selector]

train.head()

"""### Build Auto ARIMA Model at Genre Level"""

# Fit auto_arima on train set
model = pm.auto_arima(train.visitors, start_p = 1, start_q = 1,
                          max_p = 3, max_q = 3, m = 7,
                          seasonal = False,
                          d = None, trace = True,
                          error_action ='ignore',
                          suppress_warnings = True,
                          stepwise = True)

# To print the summary
model.summary()

"""#### Make Predictions"""

# Get prediction for test duration
predictions = pd.Series(model.predict(len(test)))
actuals = test['visitors'].reset_index(drop = True)

"""Let's compute various evaluation metrices now
- Mean Absolute Error
- RMSLE
- Mean Absolute Percentage Error
"""

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(predictions, actuals))
print("\n RMSLE : \n", RMSLE(predictions, actuals))
print("\n MAPE : \n", mean_absolute_percentage_error(predictions, actuals))

"""#### Plot"""

# plot predictions and actual values
predictions.plot(legend = True, label = "Prediction", xlabel = "Index", ylabel = "Visitors",  figsize=(10, 7))
actuals.plot(legend = True, label = "Actual");

"""### **Prediction at restaurant level**

Now once you have the predictions at genre level, you need to get the predictions for every restaurant.

For that you can split the predicted visitors among all restaurants in the genre by looking at the historical share of restaurants in the genre
"""

# Get the data for Yakiniku/Korean food genre
gen_df = df_tsf.loc[df_tsf['air_genre_name'] =="Yakiniku/Korean food",]
gen_df.reset_index(drop = True, inplace = True)
gen_df.head()

"""**Total Visitors by each restaurant**"""

# Aggregate the data by air_store_id to compute the total visitors for every restaurant in the past
store_visitor_df = gen_df.groupby(['air_store_id'], as_index=False).agg({'visitors': np.sum})
store_visitor_df.head()

"""**Restaurant Share**"""

# Get the share of every restaurant in the genre
store_visitor_df['restaurant_share'] = store_visitor_df['visitors']/store_visitor_df['visitors'].sum()
store_visitor_df.head()

"""**Total Visitors every day**"""

# Create a dataframe which contains the visit date and corresponding visitors prediction
prediction_df = pd.DataFrame({'visit_date' : test['visit_date'].reset_index(drop = True),
                              'visitors'   : predictions.reset_index(drop = True)
                              })
prediction_df.head()

"""**Cartesian Merge**"""

# Merge the prediction df and store_visitor df
store_visitor_df['tmp'] = 1
prediction_df['tmp'] = 1

restaurant_prediction_df = pd.merge(prediction_df, store_visitor_df.drop('visitors', axis = 1), on=['tmp'])
restaurant_prediction_df = restaurant_prediction_df.drop('tmp', axis=1)
restaurant_prediction_df.shape

# First 50 rows of merged dataframe
restaurant_prediction_df.head(50)

"""**Visitors split by restaurant share**"""

# Split the genre level prediction to restaurant level prediction
restaurant_prediction_df['visitors_predicted'] = restaurant_prediction_df['restaurant_share']*restaurant_prediction_df['visitors']
restaurant_prediction_df.head()

"""**Actual visitors for each restaurant**"""

# y actual
restaurant_df_actual = gen_df.loc[gen_df.visit_date >= '2017-02-01', ['air_store_id', 'visit_date', 'visitors']]
restaurant_df_actual.reset_index(drop = True, inplace = True)
restaurant_df_actual.head()

"""**Evaluate**"""

# evaluation_df
evaluation_df = pd.merge(restaurant_prediction_df.drop('visitors', axis = 1), restaurant_df_actual, on = ['air_store_id', 'visit_date'])
evaluation_df.head()

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(evaluation_df.visitors_predicted, evaluation_df.visitors))
print("\n RMSLE : \n", RMSLE(evaluation_df.visitors_predicted, evaluation_df.visitors))
print("\n MAPE : \n", mean_absolute_percentage_error(evaluation_df.visitors_predicted, evaluation_df.visitors))









"""## **14. Build Auto ARIMA - SARIMA model**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>14. Build Auto ARIMA - SARIMA model</h2>
</div>

### **SARIMA Modeling at Genre Level and then split Restaurants share**

Let's build a Seasonal ARIMA model at genre level and then go to restaurant level.
"""

train.head()

# Fit auto_arima on train set
model = pm.auto_arima(train.visitors, start_p = 1, start_q = 1,
                      max_p = 3, max_q = 3, m = 7,
                      start_P = 0, seasonal = True,
                      d = None, D = 1, trace = True,
                      error_action ='ignore',
                      suppress_warnings = True,
                      stepwise = True,  max_D=2, max_order=5)

# To print the summary
model.summary()

# Get prediction for test duration
predictions = pd.Series(model.predict(len(test)))
actuals = test['visitors'].reset_index(drop = True)

"""Let's compute various evaluation metrices now
- Mean Absolute Error
- RMSLE
- Mean Absolute Percentage Error
"""

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(predictions, actuals))
print("\n RMSLE : \n", RMSLE(predictions, actuals))
print("\n MAPE : \n", mean_absolute_percentage_error(predictions, actuals))

# plot predictions and actual values
predictions.plot(legend = True, label = "Prediction", xlabel = "Index", ylabel = "Visitors",  figsize=(10, 7))
actuals.plot(legend = True, label = "Actual");

"""#### Prediction at restaurant level"""

# Create a dataframe which contains the visit date and corresponding visitors prediction
prediction_df = pd.DataFrame({'visit_date' : test['visit_date'].reset_index(drop = True),
                              'visitors'   : predictions.reset_index(drop = True)
                              })
prediction_df.head()

# Merge the prediction df and store_visitor df
store_visitor_df['tmp'] = 1
prediction_df['tmp'] = 1

restaurant_prediction_df = pd.merge(prediction_df, store_visitor_df.drop('visitors', axis = 1), on=['tmp'])
restaurant_prediction_df = restaurant_prediction_df.drop('tmp', axis=1)
restaurant_prediction_df.shape

# First 50 rows of merged dataframe
restaurant_prediction_df.head(50)

# Split the genre level prediction to restaurant level prediction
restaurant_prediction_df['visitors_predicted'] = restaurant_prediction_df['restaurant_share']*restaurant_prediction_df['visitors']

restaurant_prediction_df.head()

# evaluation_df
evaluation_df = pd.merge(restaurant_prediction_df.drop('visitors', axis = 1), restaurant_df_actual, on = ['air_store_id', 'visit_date'])
evaluation_df.head()

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(evaluation_df.visitors_predicted, evaluation_df.visitors))
print("\n RMSLE : \n", RMSLE(evaluation_df.visitors_predicted, evaluation_df.visitors))
print("\n MAPE : \n", mean_absolute_percentage_error(evaluation_df.visitors_predicted, evaluation_df.visitors))

"""### **SARIMA Modeling directly at Restaurant Level**"""

air_store_id = df_tsf.air_store_id.unique()
air_store_id[:10]

# CREATE TRAIN AND TEST DATA FOR PARTICULAR RESTAURANT
df_tsf_restaurant = df_tsf.loc[df_tsf['air_store_id']==air_store_id[4], :]

# Get the train dataset, we will use the data till end of Jan 2017 for training
train_selector = df_tsf_restaurant.visit_date < '2017-02-01'
train = df_tsf_restaurant[train_selector]

test_selector = df_tsf_restaurant.visit_date >= '2017-02-01'
test = df_tsf_restaurant[test_selector]

df_tsf_restaurant.reset_index(drop=True, inplace=True)
df_tsf_restaurant.head()

# Plot
df_tsf_restaurant.visitors.plot(figsize=(12, 7));

# Fit auto_arima on train set
# Fit auto_arima on train set
# model = pm.auto_arima(train.visitors,
#                       start_p = 1,
#                       start_q = 1,
#                       max_p = 3,
#                       max_q = 3,
#                       m = 7,
#                       start_P = 0,
#                       seasonal = True,
#                       d = None,
#                       D = 1,
#                       trace = True,
#                       error_action ='ignore',
#                       suppress_warnings = True,
#                       stepwise = True,
#                       max_D=2,
#                       max_order=5)

model = pm.auto_arima(train.visitors,
                      start_p = 1,
                      start_q = 1,
                      max_p = 5,
                      max_q = 3,
                      seasonal = True,
                      m = 7,
                      d = 1,
                      # max_d = 3,
                      trace = True,
                      error_action ='ignore',
                      suppress_warnings = True,
                      start_P = 1,
                      start_Q = 1,
                      max_P = 3,
                      max_Q = 2,
                      D = 1,  # Force Seasonal Differencing
                      max_order = 7,
                      stepwise = True)

# To print the summary
model.summary()

"""#### Predict"""

# Get prediction for test duration
predictions = pd.Series(model.predict(len(test)))
predictions = predictions.map(lambda x: x if x >= 0 else 0)

actuals = test['visitors'].reset_index(drop = True)

"""#### Evaluate"""

# Evaluation Metric
print("\n MAE : \n ",  mean_absolute_error(predictions, actuals))
print("\n RMSLE : \n", RMSLE(predictions, actuals))
print("\n MAPE : \n",  mean_absolute_percentage_error(predictions, actuals))

"""#### Plot"""

# plot predictions and actual values
predictions.plot(legend = True, label = "Prediction", xlabel = "Index", ylabel = "Visitors",  figsize=(10, 7))
actuals.plot(legend = True, label = "Actual");

"""### **Forecast and Evaluate for Multiple Restaurants**"""

# CREATE TRAIN AND TEST DATA FOR PARTICULAR RESTAURANT
mapes, rmsles, maes = [], [], []

for i in range(12):
    print("==============================================")
    print(air_store_id[i]);
    df_tsf_restaurant = df_tsf.loc[df_tsf['air_store_id']==air_store_id[i], :]

    # Get the train dataset, we will use the data till end of Jan 2017 for training
    train_selector = df_tsf_restaurant.visit_date < '2017-02-01'
    train = df_tsf_restaurant[train_selector]

    test_selector = df_tsf_restaurant.visit_date >= '2017-02-01'
    test = df_tsf_restaurant[test_selector]

    # Fit auto_arima on train set
    model = pm.auto_arima(train.visitors, start_p = 1, start_q = 1,
                            max_p = 3, max_q = 3, m = 7,
                            seasonal = True,
                            d = None, trace = True,
                            error_action ='ignore',
                            suppress_warnings = True,
                            stepwise = True)

    # To print the summary
    model.summary()


    # Get prediction for test duration
    predictions = pd.Series(model.predict(len(test)))
    predictions = predictions.map(lambda x: x if x >= 0 else 0)

    actuals = test['visitors'].reset_index(drop = True)

    # Evaluation Metric
    mape  = mean_absolute_percentage_error(predictions, actuals)
    rmsle = RMSLE(predictions, actuals)
    mae = mean_absolute_error(predictions, actuals)

    print("\n MAPE : \n", mean_absolute_percentage_error(predictions, actuals))
    print("\n MAE : \n ", mean_absolute_error(predictions, actuals))
    print("\n RMSLE : \n", RMSLE(predictions, actuals))

    mapes.append(round(mape, 2))
    rmsles.append(rmsle)
    maes.append(mae)

    # plot predictions and actual values
    predictions.plot(legend = True, label = "Prediction", xlabel = "Index", ylabel = "Visitors",  figsize=(10, 7))
    actuals.plot(legend = True, label = "Actual");
    plt.title(f"MAPE: {mape}")
    plt.show()

"""**Replace Infinite with NaN values, to calculate mean**"""

mapes_clean = pd.Series(mapes).map(lambda x: np.nan if np.isinf(x) else x).values.round(2)
maes_clean = pd.Series(maes).map(lambda x: np.nan if np.isinf(x) else x).values.round(2)
rmsles_clean = pd.Series(rmsles).map(lambda x: np.nan if np.isinf(x) else x).values.round(2)

"""Calculate mean"""

print("MAPES : ", np.nanmean(mapes_clean).round(2), mapes_clean)
print("MAES  : ", np.nanmean(maes_clean).round(2), maes_clean)
print("RMSLES: ", np.nanmean(rmsles_clean).round(2), rmsles_clean)















"""## **15. Build Auto ARIMA - SARIMAX model**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>15. Build Auto ARIMA - SARIMAX model</h2>
</div>

- Seasonality = 7 days in a week

We will use 'Holiday Flag' as the external regressor here, but practically it can be any of the X variables.

### **Types of External Regressors**

1. Time Based
2. Demographics: People and Location based
3. Qualitative / Rating based
4. Promotions
5. Series Decomposition
6. Macroeconomic Data
"""

train.head()

# Fit auto_arima on train set
model = pm.auto_arima(train.visitors,
                      start_p = 1,
                      start_q = 1,
                      max_p = 3,
                      max_q = 3,
                      m = 7,
                      start_P = 0,
                      seasonal = True,
                      d = None,
                      max_D = 1,
                      trace = True,
                      error_action ='ignore',
                      suppress_warnings = True,
                      stepwise = True,
                      max_order=7,
                      X=train.loc[:, 'holiday_flg'].values.reshape(-1, 1))


# To print the summary
model.summary()

# Get prediction for test duration
predictions = pd.Series(model.predict(n_periods=len(test),
                                      X=test.loc[:, 'holiday_flg'].values.reshape(-1, 1)))

actuals = test['visitors'].reset_index(drop = True)

"""Let's compute various evaluation metrices now
- Mean Absolute Error
- RMSLE
- Mean Absolute Percentage Error
"""

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(predictions, actuals))
print("\n RMSLE : \n", RMSLE(predictions, actuals))
print("\n MAPE : \n", mean_absolute_percentage_error(predictions, actuals))

# plot predictions and actual values
predictions.plot(legend = True, label = "Prediction", xlabel = "Index", ylabel = "Visitors")
actuals.plot(legend = True, label = "Actual")

"""### Include multiple external regressors"""

# Fit auto_arima on train set
model = pm.auto_arima(train.visitors,
                      start_p = 1,
                      start_q = 1,
                      max_p = 3,
                      max_q = 3,
                      m = 7,
                      start_P = 0,
                      seasonal = True,
                      d = None,
                      max_D = 1,
                      trace = True,
                      error_action ='ignore',
                      suppress_warnings = True,
                      stepwise = True,
                      max_order=7,
                      X=train.loc[:, ['holiday_flg',
                                      'tomorrow_is_holiday',
                                      'yesterday_is_holiday']].values)


# To print the summary
model.summary()

# Get prediction for test duration
predictions = pd.Series(model.predict(n_periods=len(test),
                                      X=test.loc[:, ['holiday_flg',
                                                     'tomorrow_is_holiday',
                                                     'yesterday_is_holiday']].values))

actuals = test['visitors'].reset_index(drop = True)

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(predictions, actuals))
print("\n RMSLE : \n", RMSLE(predictions, actuals))
print("\n MAPE : \n", mean_absolute_percentage_error(predictions, actuals))

# plot predictions and actual values
predictions.plot(legend = True, label = "Prediction", xlabel = "Index", ylabel = "Visitors")
actuals.plot(legend = True, label = "Actual")

"""### Prediction at restaurant level"""

# Create a dataframe which contains the visit date and corresponding visitors prediction
prediction_df = pd.DataFrame({'visit_date' : test['visit_date'].reset_index(drop = True),
                              'visitors'   : predictions.reset_index(drop = True)
                              })
prediction_df.head()

# Merge the prediction df and store_visitor df
store_visitor_df['tmp'] = 1
prediction_df['tmp'] = 1

restaurant_prediction_df = pd.merge(prediction_df, store_visitor_df.drop('visitors', axis = 1), on=['tmp'])
restaurant_prediction_df = restaurant_prediction_df.drop('tmp', axis=1)
restaurant_prediction_df.shape

# First 50 rows of merged dataframe
restaurant_prediction_df.head(50)

# Split the genre level prediction to restaurant level prediction
restaurant_prediction_df['visitors_predicted'] = restaurant_prediction_df['restaurant_share']*restaurant_prediction_df['visitors']

restaurant_prediction_df.head()

# evaluation_df
evaluation_df = pd.merge(restaurant_prediction_df.drop('visitors', axis = 1), restaurant_df_actual, on = ['air_store_id', 'visit_date'])
evaluation_df.head()

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(evaluation_df.visitors_predicted, evaluation_df.visitors))
print("\n RMSLE : \n", RMSLE(evaluation_df.visitors_predicted, evaluation_df.visitors))
print("\n MAPE : \n", mean_absolute_percentage_error(evaluation_df.visitors_predicted, evaluation_df.visitors))

"""### **Forecast Multiple Restaurants using SARIMAX for each Restaurant**

"""

# CREATE TRAIN AND TEST DATA FOR PARTICULAR RESTAURANT
mapes, rmsles, maes = [], [], []

for i in range(12):
    print("==============================================")
    print(air_store_id[i]);
    df_tsf_restaurant = df_tsf.loc[df_tsf['air_store_id']==air_store_id[i], :]

    # Get the train dataset, we will use the data till end of Jan 2017 for training
    train_selector = df_tsf_restaurant.visit_date < '2017-02-01'
    train = df_tsf_restaurant[train_selector]
    test_selector = df_tsf_restaurant.visit_date >= '2017-02-01'
    test = df_tsf_restaurant[test_selector]


    # Fit auto_arima on train set
    model = pm.auto_arima(train.visitors,
                        start_p = 1,
                        start_q = 1,
                        max_p = 3, max_q = 3, m = 30,
                        seasonal = True,
                        d = None, trace = True,
                        error_action ='ignore',
                        suppress_warnings = True,
                        stepwise = True,
                        X=train.loc[:, 'holiday_flg'].values.reshape(-1, 1))

    # To print the summary
    model.summary()


    # Get prediction for test duration
    predictions = pd.Series(model.predict(n_periods=len(test),
                                        X=test.loc[:, 'holiday_flg'].values.reshape(-1, 1)))
    predictions = predictions.map(lambda x: x if x >= 0 else 0)
    actuals = test['visitors'].reset_index(drop = True)

    # Evaluation Metric
    mape  = mean_absolute_percentage_error(predictions, actuals)
    rmsle = RMSLE(predictions, actuals)
    mae = mean_absolute_error(predictions, actuals)

    print("\n MAPE : \n", mean_absolute_percentage_error(predictions, actuals))
    print("\n MAE : \n ", mean_absolute_error(predictions, actuals))
    print("\n RMSLE : \n", RMSLE(predictions, actuals))

    mapes.append(round(mape, 2))
    rmsles.append(rmsle)
    maes.append(mae)

    # plot predictions and actual values
    predictions.plot(legend = True, label = "Prediction", xlabel = "Index", ylabel = "Visitors",  figsize=(10, 5))
    actuals.plot(legend = True, label = "Actual");
    plt.title(f"MAPE: {mape}")
    plt.show()

"""**Replace Inf with NaN, so as to calculate mean**"""

mapes_clean = pd.Series(mapes).map(lambda x: np.nan if np.isinf(x) else x).values.round(2)
maes_clean = pd.Series(maes).map(lambda x: np.nan if np.isinf(x) else x).values.round(2)
rmsles_clean = pd.Series(rmsles).map(lambda x: np.nan if np.isinf(x) else x).values.round(2)

"""Calculate Mean"""

print("MAPES : ", np.nanmean(mapes_clean).round(2), mapes_clean)
print("MAES  : ", np.nanmean(maes_clean).round(2), maes_clean)
print("RMSLES: ", np.nanmean(rmsles_clean).round(2), rmsles_clean)

"""## **15b Types of Exogenous Variables**

1. **Time Based**
 - Holiday or Not
 - Weekday / Week End
 - Yesterday was holiday
 - Tomorrow is holiday
 - Day of the week
 - Month of the year
 - Hour of the day
 - Number of Working days


2. **Demographics: People - Location Based**
 - Population Density
 - Percentage of Age Group ('0-3', ' less than 12', "less than 18", '<=40', '<=60'. '60+')
 - Presence of Public Transport (Bus, Railway stations)
 - Landmark Locations (Stadium, Temples, Churches, Mosques)
 - Number of competitors nearby
 - Average Income
 - Proportion by Ethnicity
 - Zipcode
 - Average family income
 - Number of cars owned
 - Number of voters
 - Percentage of buyers
 - Wallet share



3. **Qualitative / Rating**
 - Customer ratings on products
 - Presence of competitive advantage
 - Restaurant features (AC / Wifi)
 - Handling / Seating Capacity
 - Average review rating on Yelp / Zomato etc


4. **Promotions Information**
 - Type of promotional campaign
   - Buy 1 Get 1
   - Flat x% off
   - Limited Time / Limited Quantity discount
 - The Percentage of Discount Given
 - The Dollar value of discount given
 - Advertising Spend:
   - TV ads
   - Radio
   - Banner
   - National News
   - National News paper
   - Local News Paper
   - Store Front Ads
   - Long Term / Brand Advertising
   - Advertising growth
   - Short Term Sales Promotions



5. **Series Decomposition Signals and LifeCycle Features**
 - Seasonal Index
 - Trend
 - Age since launch
 - Age since Facelift
 - Fourier Transforms
 - Life Cycle Features

**Fourier_Theorem**: A mathematical theorem stating that a PERIODIC function f(x) which is reasonably continuous may be expressed as the sum of a series of sine or cosine terms (called the Fourier series), each of which has specific AMPLITUDE and PHASE coefficients known as Fourier coefficients.




6. **Macro Economic data**
 - Inflation Rate
 - CPI (Consumer Price Index)
 - WPI (Wholesale Price Index)
 - Housing Starts
 - Crude Oil Prices
 - Metal Prices
 - Central Bank Interest Rate
 - GDP Growth
 - Population Growth
 - Personal Income Tax Rate
 - Sales Tax Rate
 - Many more from various domain / country specific sources



**Sources  of Macro Economic, Demographic Data**
 - Acxiom
 - OECD Stat
 - Dun and Bradstreet
 - IHS
 - JATO
 - World Bank
 - JD Power (Automotive)

## **16. Forecast on submission dataset**

Need to map the X variable to submission data and then predict.
"""

# filter submission dataset
df_submission = df_univariate.loc[df_univariate.dataset == "future", ]
df_submission.head()

# Duration of submission data
duration = datetime.datetime.strptime(max(df_submission.visit_date), '%Y-%m-%d') - datetime.datetime.strptime(min(df_submission.visit_date), '%Y-%m-%d')
duration = duration.days + 1

"""Fit the model on complete past dataset in order to get best results on the submission dataset"""

# Fit auto_arima on train set
model = pm.auto_arima(df_genre.visitors, start_p = 1, start_q = 1,
                          max_p = 3, max_q = 3, m = 7,
                          start_P = 0, seasonal = True,
                          d = None, D = 1, trace = True,
                          error_action ='ignore',
                          suppress_warnings = True,
                          stepwise = True)

# Get prediction for submission duration
sumbission_duration = duration
predictions = pd.Series(model.predict(sumbission_duration))
predictions

"""### Prediction at restaurant level"""

# Create a dataframe which contains the visit date and corresponding visitors prediction
prediction_df = pd.DataFrame({'visit_date' : df_submission.visit_date.unique(),
                              'visitors'   : predictions.reset_index(drop = True)
                              })
prediction_df.head()

# Merge the prediction df and store_visitor df
store_visitor_df['tmp'] = 1
prediction_df['tmp'] = 1

restaurant_prediction_df = pd.merge(prediction_df, store_visitor_df.drop('visitors', axis = 1), on=['tmp'])
restaurant_prediction_df = restaurant_prediction_df.drop('tmp', axis=1)
restaurant_prediction_df.shape

# First 50 rows of merged dataframe
restaurant_prediction_df.head(50)

# Split the genre level prediction to restaurant level prediction
restaurant_prediction_df['visitors_predicted'] = restaurant_prediction_df['restaurant_share']*restaurant_prediction_df['visitors']

restaurant_prediction_df.head()

"""### Restaurant level Model"""

all_store_ids = df_tsf['air_store_id'].unique()
all_store_ids[:10]

df_restaurant = df_tsf.loc[df_tsf['air_store_id'] =="air_ba937bf13d40fb24",]
df_restaurant.reset_index(drop = True, inplace = True)
df_restaurant.head()

# Get the train dataset, we will use the data till end of Jan 2017 for training
train_selector = df_restaurant.visit_date < '2017-02-01'
train = df_restaurant[train_selector]
test_selector = df_restaurant.visit_date >= '2017-02-01'
test = df_restaurant[test_selector]

train.head()

"""### Build Auto SARIMA model

- Seasonality = 7
"""

# Fit auto_arima on train set
model = pm.auto_arima(train.visitors, start_p = 1, start_q = 1,
                          max_p = 3, max_q = 3, m = 7,
                          start_P = 0, seasonal = True,
                          d = None, D = 1, trace = True,
                          error_action ='ignore',
                          suppress_warnings = True,
                          stepwise = True)


# To print the summary
model.summary()

# Get prediction for test duration
predictions = pd.Series(model.predict(len(test)))
actuals = test['visitors'].reset_index(drop = True)

"""Let's compute various evaluation metrices now
- Mean Absolute Error
- RMSLE
- Mean Absolute Percentage Error
"""

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(predictions, actuals))
print("\n RMSLE : \n", RMSLE(predictions, actuals))
print("\n MAPE : \n", mean_absolute_percentage_error(predictions, actuals))

# plot predictions and actual values
predictions.plot(legend = True, label = "Prediction", xlabel = "Index", ylabel = "Visitors")
actuals.plot(legend = True, label = "Actual")

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- The MAPE is 50%
- The RMSLE is 0.75

Rather than training the models on every restaurant, let's train the model for 10 restaurants and check the error and check if it's feasible to use ARIMA for predictions

"""

# Create an empty dict to store the evaluation metrics for every model
eval_metrics = {'MAE'   : [],
                'MAPE'  : []}

# Note : I am not using RMSLE here as there might be some negative predictions as well and RMSLE won't work in case of negative predictions

# Train the model for first 10 store ids and get the predictions and error value
for id in all_store_ids[:10]:
    print( "Restaurant ", id)
    df_restaurant = df_tsf.loc[df_tsf['air_store_id'] == id,]
    df_restaurant.reset_index(drop = True, inplace = True)

    # Get the train dataset, we will use the data till end of Jan 2017 for training
    train_selector = df_restaurant.visit_date < '2017-02-01'
    train = df_restaurant[train_selector]
    test_selector = df_restaurant.visit_date >= '2017-02-01'
    test = df_restaurant[test_selector]

    # Fit auto_arima on train set
    model = pm.auto_arima(train.visitors, start_p = 1, start_q = 1,
                            max_p = 3, max_q = 3, m = 7,
                            start_P = 0, seasonal = True,
                            d = None, D = 1, trace = True,
                            error_action ='ignore',
                            suppress_warnings = True,
                            stepwise = True)

    # Get prediction for test duration
    predictions = pd.Series(model.predict(len(test)))
    # Actuals
    actuals = test['visitors'].reset_index(drop = True)

    # Evaluation Metric
    eval_metrics['MAE'].append(mean_absolute_error(predictions, actuals))
    eval_metrics['MAPE'].append(mean_absolute_percentage_error(predictions, actuals))

# Print the evaluation metrics
print("\n MAE : \n", eval_metrics['MAE'])
print("\n MAPE : \n", eval_metrics['MAPE'])

"""The error is very high.

- We have to predict the number of visitors visiting in restaurant on a daily basis. With univariate forecasting, it won't be that easy as lot of variables play an essential role in predicting number of visitors in restaurant. Additionally this process will be very slow as well.

So let's look at machine learning models for that.

## **17. Time Series Forecasting - Prophet by Facebook**
"""

# Install
# !pip install prophet

from fbprophet import Prophet

# Get the train dataset for a air genre, we will use the data till end of Jan 2017 for training
train_selector = df_genre.visit_date < '2017-02-01'
train = df_genre[train_selector]
test_selector = df_genre.visit_date >= '2017-02-01'
test = df_genre[test_selector]

df = train[["visit_date","visitors"]]

# prepare expected column names
df.columns = ['ds', 'y']
df['ds']= pd.to_datetime(df['ds'])
df.head(10)

# Fit the model
model = Prophet()
model.fit(df)
model

model.changepoints

# Get the test datset in date time format
future_df = pd.DataFrame({'ds' : test.visit_date})
future_df['ds'] = pd.to_datetime(future_df['ds'])

# Get predictions
forecast = model.predict(future_df)
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()

# Python
from prophet.plot import add_changepoints_to_plot
fig = model.plot(forecast)
a = add_changepoints_to_plot(fig.gca(), model, forecast)

# Visualize the forecast
fig1 = model.plot(forecast)

# Visualize the components of forecast
fig2 = model.plot_components(forecast)

"""### Evaluate"""

# Evaluation Metric
predictions = forecast['yhat']
actuals = test['visitors'].reset_index(drop = True)
print("\n MAE : \n ", mean_absolute_error(predictions, actuals))
print("\n RMSLE : \n", RMSLE(predictions, actuals))
print("\n MAPE : \n", mean_absolute_percentage_error(predictions, actuals))

actuals.index = pd.to_datetime(test.visit_date)
predictions.index = pd.to_datetime(test.visit_date)

# plot predictions and actual values
import matplotlib.pyplot as plt
predictions.plot(legend = True,
                 label = "Prediction",
                 xlabel = "Index",
                 ylabel = "Visitors")

actuals.plot(legend = True, label = "Actual")

"""### **Forecast with External Regressors**"""

# Get the train dataset for a air genre, we will use the data till end of Jan 2017 for training
train_selector = df_genre.visit_date < '2017-02-01'
train = df_genre[train_selector]

test_selector = df_genre.visit_date >= '2017-02-01'
test = df_genre[test_selector]

train.head()

# Include the External Regressors in the datasets
df_train = train[["visit_date", "visitors", "holiday_flg", "tomorrow_is_holiday", "yesterday_is_holiday"]]
df_test = test[["visit_date", "visitors", "holiday_flg", "tomorrow_is_holiday", "yesterday_is_holiday"]]


# prepare expected column names
df_train.columns = ['ds', 'y', "holiday_flg", "tomorrow_is_holiday", "yesterday_is_holiday"]
df_test.columns = ['ds', 'y', "holiday_flg", "tomorrow_is_holiday", "yesterday_is_holiday"]

df_train['ds']= pd.to_datetime(df_train['ds'])
df_test['ds']= pd.to_datetime(df_test['ds'])

df_test.head(10)

# Fit the model
model = Prophet()
model.add_regressor("holiday_flg")
model.add_regressor("tomorrow_is_holiday")
model.add_regressor("yesterday_is_holiday")
model.fit(df_train)
model

model.changepoints

# Get the test datset in date time format
future_df = df_test.copy()

# future_df = pd.DataFrame({'ds' : test.visit_date})
# future_df['ds'] = pd.to_datetime(future_df['ds'])

# Get predictions
forecast = model.predict(future_df)
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()

# Visualize the forecast
fig1 = model.plot(forecast)

# Visualize the components of forecast
fig2 = model.plot_components(forecast)

"""### Evaluate"""

# Evaluation Metric
predictions = forecast['yhat']
actuals = test['visitors'].reset_index(drop = True)
print("\n MAE : \n ", mean_absolute_error(predictions, actuals))
print("\n RMSLE : \n", RMSLE(predictions, actuals))
print("\n MAPE : \n", mean_absolute_percentage_error(predictions, actuals))

# plot predictions and actual values
predictions.plot(legend = True, label = "Prediction", xlabel = "Index", ylabel = "Visitors")
actuals.plot(legend = True, label = "Actual")

"""### Prediction at restaurant level"""

# Create a dataframe which contains the visit date and corresponding visitors prediction
prediction_df = pd.DataFrame({'visit_date' : test['visit_date'].reset_index(drop = True),
                              'visitors'   : predictions.reset_index(drop = True)
                              })
prediction_df.head()

# Merge the prediction df and store_visitor df
store_visitor_df['tmp'] = 1
prediction_df['tmp'] = 1

restaurant_prediction_df = pd.merge(prediction_df, store_visitor_df.drop('visitors', axis = 1), on=['tmp'])
restaurant_prediction_df = restaurant_prediction_df.drop('tmp', axis=1)
restaurant_prediction_df.shape

# First 50 rows of merged dataframe
restaurant_prediction_df.head(50)

# Split the genre level prediction to restaurant level prediction
restaurant_prediction_df['visitors_predicted'] = restaurant_prediction_df['restaurant_share']*restaurant_prediction_df['visitors']

restaurant_prediction_df.head()

# evaluation_df
evaluation_df = pd.merge(restaurant_prediction_df.drop('visitors', axis = 1), restaurant_df_actual, on = ['air_store_id', 'visit_date'])
evaluation_df.head()

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(evaluation_df.visitors_predicted, evaluation_df.visitors))
print("\n RMSLE : \n", RMSLE(evaluation_df.visitors_predicted, evaluation_df.visitors))
print("\n MAPE : \n", mean_absolute_percentage_error(evaluation_df.visitors_predicted, evaluation_df.visitors))

"""### Prediction for submission dataset"""

# Get the full training data
df = df_genre[["visit_date","visitors"]]

# prepare expected column names
df.columns = ['ds', 'y']
df['ds']= pd.to_datetime(df['ds'])
df.head()

# Fit the model
model = Prophet()
model.fit(df)
model

# filter submission dataset
df_submission = df_univariate.loc[df_univariate.dataset == "future",]
df_submission.head()

df_submission.visit_date.unique()

# Get the test datset in date time format
future_df = pd.DataFrame({'ds' : df_submission.visit_date.unique()})
future_df['ds'] = pd.to_datetime(future_df['ds'])

# Get predictions
forecast = model.predict(future_df)
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]

"""## **Machine Learning Models**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>Machine Learning Models</h2>
</div>

## **18. XGB Regressor**

XGBoost is an optimized distributed gradient boosting model designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.

`xgboost.XGBRegressor` is the wrappers that prepare the DMatrix and pass in the corresponding objective function and parameters.
"""

from xgboost.sklearn import XGBRegressor

X_train.head()

# Convert Y back to original scale
y_train_ = y_train.apply(np.exp) - 1
y_test_ = y_test.apply(np.exp) - 1
print(y_train_.head())

X_train.head()

# Define the model
xgbr = XGBRegressor(n_jobs = -1, random_state = 100)

# Train the model
xgbr.fit(X_train.iloc[:,3:], y_train_)

"""Let's use the model to get predictions on test dataset."""

# Prediction
y_pred = xgbr.predict(X_test.iloc[:,3:])
y_pred

"""Let's compute various evaluation metrices now
- Mean Absolute Error
- RMSLE
- Mean Absolute Percentage Error
"""

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(y_pred, y_test_))
print("\n RMSLE : \n", RMSLE(y_pred, y_test_))
print("\n MAPE : \n", mean_absolute_percentage_error(y_pred, y_test_))

"""### Feature Importance
Feature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable.
"""

# Feature Importance
feature_importance_df = pd.DataFrame({'feature' : X_train.iloc[:,3:].columns, 'importance' : xgbr.feature_importances_ })
feature_importance_df = feature_importance_df.sort_values(by="importance", ascending=False)
feature_importance_df = feature_importance_df.iloc[:30,:]
feature_importance_df

# Plot feature importance
plt.figure(figsize=(16, 12));
sns.barplot(x="importance", y="feature", data=feature_importance_df.sort_values(by="importance", ascending=False));
plt.title('XGBRegressor Features');

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- With XGBRegressor, we got RMSLE value of 0.15 and MAPE value of 14%.
- Only 2 features are dominating the feature importance metric, so this is not a good model.


Let's look at a XGB model, **not XGBRegressor**

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>XGBoost Demo</h2>
</div>

XGBoost is an optimized distributed gradient boosting model designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.


xgboost.train is the low-level API to train the model via gradient boosting method.
"""

import xgboost as xgb

"""**Initialize parameters**"""

# Initialize parameters for XGBoost model
# Ref: https://xgboost.readthedocs.io/en/latest/parameter.html

param = {'colsample_bytree': 0.4,  # range:(0,1] -  subsample ratio of columns when constructing each tree. Default: 1
         'eta': 0.1,               # range: [0, 1] - Learning rate. Default 0.3
         'gamma': 2,               # range: [0, inf] - Minimum loss reduction req to further partition a leaf node. Def: 0
         'max_depth': 4,           # range: [0, inf] - Maximum depth of a tree. default: 6
         'min_child_weight': 100,  # range: [0, inf] - Minimum sum of instance weight needed in a child. Default: 0
         'objective': 'reg:squarederror',  # regression with squred loss
         'seed': 2018,
         'subsample': 1,           # Subsample ratio of the training instance. Default: 1
         'n_jobs' : -1}            # number of parallel jobs

"""**Training Data**

"""

X_train.head()

"""**Convert X back to original scale**"""

# Convert Y back to original scale
y_train_ = y_train.apply(np.exp) - 1
y_test_ = y_test.apply(np.exp) - 1
print(y_train_.head())

"""**Create DMatrix for training**"""

# Create DMatrix for training and testing dataset
train_dmatrix = xgb.DMatrix(X_train.iloc[:, 2:], label=y_train_)
test_dmatrix = xgb.DMatrix(X_test.iloc[:, 2:], label=y_test_)

# Define the evaluation list
evallist = [(train_dmatrix, 'train'), (test_dmatrix, 'eval')]

"""**Train the model**"""

# Train the model
model = xgb.train(params=param,
                  dtrain=train_dmatrix,
                  num_boost_round=100000,
                  # evals=evallist,
                  early_stopping_rounds=50,
                  verbose_eval=100
                 )

best_iteration = model.best_iteration
best_score = model.best_score

# Print best score and best iteration
print("Best score : ", best_score)
print("Best iteration : ", best_iteration)

"""Let's use the model to get predictions on test dataset."""

# Prediction
y_pred = model.predict(test_dmatrix)
y_pred

"""Let's compute various evaluation metrices now
- Mean Absolute Error
- RMSLE
- Mean Absolute Percentage Error
"""

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(y_pred, y_test_))
print("\n RMSLE : \n", RMSLE(y_pred, y_test_))
print("\n MAPE : \n", mean_absolute_percentage_error(y_pred, y_test_))

"""#### Feature Importance
Feature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable.
"""

# Feature Importance
feature_importance_df             = pd.DataFrame.from_dict(model.get_score(importance_type='gain'), orient='index')
feature_importance_df.columns     = ['importance']
feature_importance_df['feature']  = feature_importance_df.index
feature_importance_df             = feature_importance_df.sort_values(by='importance', ascending=False).head(30)
feature_importance_df

# Plot feature importance
plt.figure(figsize=(16, 12));
sns.barplot(x="importance", y="feature", data=feature_importance_df.sort_values(by="importance", ascending=False));
plt.title('XGB Feature Importance');

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- With XGB, we got RMSLE value of 0.56 and MAPE value of 17.61% which is almost similar to XGBRegressor
- 4-5 features are contributing in predicting the y variable, it's better than XGBRegressor but not upto the mark


Let's look at a CatBoost model to see if the performance can be improved

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>CatBoost Demo</h2>
</div>

CatBoost is a high-performance open source library for gradient boosting on decision trees

Features of CatBoost:

- Great Quality without parameter tuning
- Categorical feature support
- Fast and scalable GPU version
- Improved accuracy
- Fast prediction
"""

# !pip install catboost

from catboost import CatBoostRegressor

X_train.head()

"""**Transform Y back to original scale**"""

# Convert Y back to original scale
y_train_ = y_train.apply(np.exp) - 1
y_test_ = y_test.apply(np.exp) - 1
print(y_train_.head())

"""**Train the model**"""

# Define the model
catboost_regressor = CatBoostRegressor(random_state = 100,
                                       loss_function='RMSE'  # default
                                       )

# Train the model
catboost_regressor.fit(X=X_train.iloc[:, 2:],
                       y=y_train_,
                       plot=True,
                       # eval_set = (X_test.iloc[:,2:], y_test_)
                       )

"""Let's use the model to get predictions on test dataset."""

# Prediction
y_pred = catboost_regressor.predict(X_test.iloc[:,2:])
y_pred

"""Let's compute various evaluation metrices now
- Mean Absolute Error
- RMSLE
- Mean Absolute Percentage Error
"""

# Replace negative values with 1
y_pred = np.where(y_pred < 0, 1, y_pred)

# check  number of replacements
np.sum(y_pred == 1)

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(y_pred, y_test_))
print("\n RMSLE : \n", RMSLE(y_pred, y_test_))
print("\n MAPE : \n", mean_absolute_percentage_error(y_pred, y_test_))

"""#### Feature Importance
Feature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable.
"""

# Feature Importance
feature_importance_df = pd.DataFrame({'feature' : X_train.iloc[:,2:].columns, 'importance' : catboost_regressor.feature_importances_})
feature_importance_df = feature_importance_df.sort_values(by="importance", ascending=False)
feature_importance_df = feature_importance_df.iloc[:30,:]
feature_importance_df

# Plot feature importance
plt.figure(figsize=(16, 12));
sns.barplot(x="importance", y="feature", data=feature_importance_df.sort_values(by="importance", ascending=False));
plt.title('CatBoostRegressor Features');

"""**Feature statistics for mean_visitors**"""

catboost_regressor.calc_feature_statistics(data=X_train.iloc[:, 2:],
                                           target=y_train_,
                                           feature='mean_visitors',
                                           plot=True,
                                           plot_file="mean_visitors.html")

"""**Feature statistics for day_of_week**"""

catboost_regressor.calc_feature_statistics(data=X_train.iloc[:, 2:],
                                           target=y_train_,
                                           feature='day_of_year',
                                           plot=True,
                                           plot_file="day_of_year.html");

"""**Feature Statistics for Week of year**"""

catboost_regressor.calc_feature_statistics(data=X_train.iloc[:, 2:],
                                           target=y_train_,
                                           feature='week_of_year',
                                           plot=True,
                                           plot_file="week_of_year.html");



























"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

**Train the model with categorical values specified**
"""

X_train.head()

cat_columns = ['day_of_week', 'holiday_flg', 'weekend',
              'day_off_flg',	'tomorrow_is_holiday',
              'yesterday_is_holiday',	'jump_flag',
              'weekday',	'year',	'month',	'day_of_year',
              'days_in_month',	'week_of_year',	'is_month_end']
              # 'air_genre_name',	'air_area_name',
              # 'hpg_store_id',	'hpg_genre_name',
              # 'hpg_area_name',	'Todofuken',	'city',
              # 'street',	'visit_hour',	'reserve_hour',
              # 'area_genre',	'store_weekday',	'store_weekday_holiday']

# Convert Categorical columns to int64 in training data
X_train_ = X_train.copy()
X_train_.fillna(0, inplace=True)
X_train_[cat_columns] = X_train_[cat_columns].astype(np.str)

# Convert Categorical columns to int64 in test data
X_test_ = X_test.copy()
X_test_.fillna(0, inplace=True)
X_test_[cat_columns] = X_test_[cat_columns].astype(np.str)

X_train_.dtypes

# Define the model:
# 1. Change Loss function
# 2. explicit categorical columns
# 3. Increase number of iterations

catboost_regressor = CatBoostRegressor(random_state = 100,
                                       # loss_function='MAPE',  # default=RMSE
                                       # cat_features = cat_columns,
                                       iterations=1500
                                       )

# Train the model
catboost_regressor.fit(X=X_train_.iloc[:,2:],
                       y=y_train_,
                       plot=True)

X_test_.dtypes

# Prediction
y_pred = catboost_regressor.predict(X_test_.iloc[:,2:])
y_pred

# Replace negative values with 1
y_pred = np.where(y_pred < 0, 1, y_pred)

# check  number of replacements
np.sum(y_pred == 1)

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(y_pred, y_test_))
print("\n RMSLE : \n", RMSLE(y_pred, y_test_))
print("\n MAPE : \n", mean_absolute_percentage_error(y_pred, y_test_))

"""<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>Improve Model</h2>
</div>

The goal of this section is to:
- Perform hyperparameter tuning to find the most optimal parameters
- Build ensemble models
- Get final recommendations

<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>22. Hyperparameter tuning</h2>
</div>


**Hyperparameter** is a parameter whose value is set before the learning process begins

**Hyperparameter tuning** refers to the automatic optimization of the hyper-parameters of a ML model
"""

from catboost import CatBoostRegressor

# Convert Y back to original scale
y_train_ = y_train.apply(np.exp) - 1
y_test_ = y_test.apply(np.exp) - 1
print(y_train_.head())

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Define the estimator
# catbregressor = CatBoostRegressor(random_state = 100)
# 
# 
# # Define the parameters gird
# param_grid = {
#     'depth'         : [6,10],       # 8
#     'learning_rate' : [0.01, 0.1],  # 0.05
#     'iterations'    : [100, 500]    # 1000
#     }
# 
# 
# # run grid search
# grid = GridSearchCV(catbregressor,
#                     param_grid=param_grid,
#                     refit = True,
#                     verbose = 3,
#                     n_jobs=-1,
#                     cv = 3)
# 
# 
# # fit the model for grid search
# grid.fit(X_train.iloc[:,2:], y_train_)

"""Get the best parameters corresponding to which you have best model"""

# Best parameter after hyper parameter tuning
print(grid.best_params_)

# Moel Parameters
print(grid.best_estimator_)

catbregressor = grid.best_estimator_

"""Let's use the best model to get predictions on test dataset."""

# Prediction
y_pred = catbregressor.predict(X_test.iloc[:, 2:])
y_pred

"""Let's compute various evaluation metrices now
- Mean Absolute Error
- RMSLE
- Mean Absolute Percentage Error
"""

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(y_pred, y_test_))
print("\n RMSLE : \n", RMSLE(y_pred, y_test_))
print("\n MAPE : \n", mean_absolute_percentage_error(y_pred, y_test_))

"""#### Feature Importance
Feature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable.
"""

# Feature Importance
feature_importance_df = pd.DataFrame({'feature' : X_train.iloc[:,2:].columns, 'importance' : catbregressor.feature_importances_ })
feature_importance_df = feature_importance_df.sort_values(by="importance", ascending=False)
feature_importance_df = feature_importance_df.iloc[:30,:]
feature_importance_df

# Plot feature importance
plt.figure(figsize=(16, 12));
sns.barplot(x="importance",
            y="feature",
            data=feature_importance_df.sort_values(by="importance", ascending=False));

plt.title('CatBoostRegressor Features');













"""<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>Cohorted Ensemble Models</h2>
</div>

- Let's build separate models based on values of various features like different model for a holiday and non holiday, different models for cities etc.
- In the end ensemble all these models to get the final prediction
"""

X_train.head()

# Convert Y back to original scale
y_train_ = y_train.apply(np.exp) - 1
y_test_ = y_test.apply(np.exp) - 1
print(y_train_.head())

"""**Training the model**"""

from catboost import CatBoostRegressor

# Define the features on which you wish to build the model
submodel = {}
submodel_by_features = ['weekday', 'holiday_flg', 'tomorrow_is_holiday', 'air_genre_name', 'Todofuken', 'city']

# Train the LGBMRegressor model for every category in given features. Store it in a dictionary
for feat in submodel_by_features:
    submodel[feat] = {}
    print('Training a sub-model for each {}'.format(feat))
    for feat_value in X_train[feat].unique():
        print('Sub-model for {}={}'.format(feat, feat_value))

        # train and val datasets are used for tuning model parameters
        train_selector = (X_train[feat] == feat_value)
        X_train_subset = X_train.loc[train_selector].iloc[:, 2:]
        y_train_subset = y_train_[train_selector]

        cbr = CatBoostRegressor(random_state = 100, loss_function='RMSE')
        submodel[feat][feat_value] = cbr.fit(X_train_subset, y_train_subset)

"""**Making Predictions**"""

# Define a function to predict the visitors based on feature category

# For each row in dataset, make predictions using each
# submodel in 'submodel_by_features'

# For each feature in submodel_by_features, the prediction is made using the
# submodel corresponding to the category value of the feature.

def pred_rowbyrow(row):
    row_df = pd.DataFrame().append(row)[X_train.columns[2:]]
    pred = {}
    for feat in submodel_by_features:
        use_model = submodel[feat][row[feat]]
        pred_colname = 'pred_by_'+str(feat)
        pred[pred_colname] = use_model.predict(row_df)[0]
    return pd.Series(pred)

X_test.reset_index(drop = True, inplace = True)
y_test_.reset_index(drop = True, inplace = True)

# Get the prediction
pred = X_test.apply(pred_rowbyrow, axis=1)
pred = pred.join(X_test.air_store_id)
pred["actual"] = y_test_

# Get the mean of all the predictions for every row
pred['pred_mean_visitor'] = pred.filter(regex='pred_').mean(axis=1)
pred.head()

"""Let's compute various evaluation metrices now
- Mean Absolute Error
- RMSLE
- Mean Absolute Percentage Error
"""

# Evaluation Metric
print("\n MAE : \n ", mean_absolute_error(pred['pred_mean_visitor'], pred["actual"]))
print("\n RMSLE : \n", RMSLE(pred['pred_mean_visitor'], pred["actual"]))
print("\n MAPE : \n", mean_absolute_percentage_error(pred['pred_mean_visitor'], pred["actual"]))

"""<div class="alert alert-info" style="padding:0px 10px; border-radius:5px;"><h3 style='margin:10px 5px'> Inferences:</h3>
</div>

- With ensemble model, we got RMSLE value of 0.153 and MAPE value of 14.1%. The performance is almost same as compared to CatBoostRegressor


The y variable and predicred values are still the logarithmic values. Let's transform it back to normal and get the final RMSLE score which is the evaluation metric for this competition/case study
"""



















"""<div class="alert alert-info" style="background-color:#006a79; color:white; padding:0px 10px; border-radius:5px;"><h2 style='margin:10px 5px'>Final Words</h2>
</div>

Congratulations!

The model has been trained and tested, so now you can use it to predict the number of visitors visiting the restaurant

One very important thing to note is that you need to keep a check on the performance of the model, if it deteriorates, re-train the model with new training data and use it for predictions

**Topics Covered:**
1. Problem Statement Understanding
2. Import data, assemble and pre-process it.
3. Extensive EDA
4. Engineering Features
5. Statistical Tests
6. Feature Encoding Techniques
7. Evaluation Metrics
8. Time Series Regression Based Approaches
   - Arima,
   - Auto Arima,
   - Prophet,
   - SARIMA & SARIMAX
9. Error Diagnostics
10. Building models at Restaurant level, Genre level
11. XGBoost
12. CatBoost
13. Hyper Parameter Tuning - Catboost
14. Cohorted Ensembles
"""

